{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a273cbd",
   "metadata": {},
   "source": [
    "# ğŸ—ï¸ Digital Construction Archive (DCA) - CSV zu RDF Pipeline fÃ¼r RenkuLab\n",
    "\n",
    "**Zweck**: Migration der DROID CSV-Daten zu RDF basierend auf DCA-Ontologie  \n",
    "**Zielumgebung**: RenkuLab Jupyter Notebook  \n",
    "**Standards**: RiC-O, PREMIS, Dublin Core mit DCA-spezifischen Erweiterungen  \n",
    "**Datum**: Februar 2026  \n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ Quick Start Guide\n",
    "1. **Kopieren Sie jede Zelle einzeln** zu RenkuLab  \n",
    "2. **FÃ¼hren Sie sie der Reihe nach aus**  \n",
    "3. **Passen Sie die Pfade** in Schritt 2 an Ihre RenkuLab-Struktur an  \n",
    "4. **Ãœberwachen Sie Output** fÃ¼r Fehler oder Warnungen  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0458e7",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Schritt 1: RenkuLab Dependencies & Core Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b260cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# FIRST RUN IN RENKULAB: Install dependencies\n",
    "# =====================================================\n",
    "# !pip install rdflib pandas exifread pillow networkx\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json, subprocess, hashlib, sys, math, re\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, List, Set, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# RDF Core Libraries\n",
    "from rdflib import Graph, Namespace, URIRef, BNode, Literal\n",
    "from rdflib.namespace import RDF, RDFS, XSD, DCTERMS\n",
    "from rdflib.plugins.serializers.turtle import TurtleSerializer\n",
    "\n",
    "# Optional: Network analysis for provenance graphs\n",
    "try:\n",
    "    import networkx as nx\n",
    "    NX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  NetworkX not available - provenance graphs disabled\")\n",
    "    NX_AVAILABLE = False\n",
    "\n",
    "print(\"âœ… All dependencies loaded successfully\")\n",
    "print(f\"ğŸ“… DCA Pipeline started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ”§ Python {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(f\"ğŸ“š RDFLib version: {getattr(Graph(), 'version', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b75bf",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Schritt 2: Digital Construction Archive (DCA) Ontology Setup\n",
    "\n",
    "**Wichtig**: Diese Namespaces entsprechen exakt dem ETH DCA Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# DCA ONTOLOGY NAMESPACES - ETH ZÃ¼rich Standard\n",
    "# =====================================================\n",
    "\n",
    "# Core DCA Namespaces\n",
    "DCA      = Namespace(\"http://dca.ethz.ch/ontology#\")     # DCA Classes & Properties\n",
    "DCA_ID   = Namespace(\"http://dca.ethz.ch/id/\")          # Individual Resources\n",
    "DCA_TECH = Namespace(\"http://dca.ethz.ch/tech#\")        # Technical Properties\n",
    "\n",
    "# International Standards\n",
    "PREMIS   = Namespace(\"http://www.loc.gov/premis/rdf/v3/\")  # Digital Preservation Metadata\n",
    "RICO     = Namespace(\"https://www.ica.org/standards/RiC/ontology#\")  # Records in Contexts\n",
    "# DCTERMS already imported from rdflib.namespace\n",
    "\n",
    "# Utility\n",
    "OWL      = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "XSDNS    = XSD  # Shorthand\n",
    "\n",
    "# =====================================================\n",
    "# DCA FILE TYPE DEFINITIONS\n",
    "# =====================================================\n",
    "\n",
    "# Image file extensions (construction documentation)\n",
    "IMG_EXTENSIONS = {\n",
    "    \"jpg\", \"jpeg\", \"tif\", \"tiff\", \"png\", \"gif\", \"bmp\", \"webp\",\n",
    "    # RAW formats (professional photography)\n",
    "    \"dng\", \"cr2\", \"cr3\", \"nef\", \"arw\", \"orf\", \"rw2\"\n",
    "}\n",
    "\n",
    "# Adobe Creative Suite formats\n",
    "ADOBE_EXTENSIONS = {\n",
    "    \"psd\", \"psb\", \"ai\", \"indd\", \"idml\", \"eps\",\n",
    "    \"pdf\"  # Often contains rich Adobe metadata\n",
    "}\n",
    "\n",
    "# CAD & Engineering formats\n",
    "CAD_EXTENSIONS = {\n",
    "    \"dwg\", \"dxf\", \"step\", \"stp\", \"iges\", \"igs\", \n",
    "    \"ifc\", \"3dm\", \"skp\"  # BIM/Architecture specific\n",
    "}\n",
    "\n",
    "# Combined target extensions for metadata enrichment\n",
    "TARGET_EXTENSIONS = IMG_EXTENSIONS | ADOBE_EXTENSIONS | CAD_EXTENSIONS\n",
    "\n",
    "print(f\"ğŸ—ï¸  DCA Ontology initialized\")\n",
    "print(f\"ğŸ“ Target file types: {len(TARGET_EXTENSIONS)} extensions\")\n",
    "print(f\"ğŸ“¸ Image types: {len(IMG_EXTENSIONS)}\")\n",
    "print(f\"ğŸ¨ Adobe types: {len(ADOBE_EXTENSIONS)}\")\n",
    "print(f\"ğŸ“ CAD types: {len(CAD_EXTENSIONS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac5f02f",
   "metadata": {},
   "source": [
    "## ğŸ”— Schritt 3: DCA ID Generation - MD5-Based Consistent Identifiers\n",
    "\n",
    "**Kritisch**: Diese Funktionen mÃ¼ssen identische IDs wie das ETZ DCA System erzeugen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc68742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# DCA ID GENERATION - MD5-BASED SYSTEM\n",
    "# =====================================================\n",
    "\n",
    "# Base URI for all DCA identifiers\n",
    "DCA_ID_BASE = \"http://dca.ethz.ch/id/\"\n",
    "\n",
    "def dca_file_uri_from_md5(md5_hex: Optional[str]) -> Optional[URIRef]:\n",
    "    \"\"\"\n",
    "    Creates dca-id:file_<md5[:16]> from MD5 hex string (DROID format).\n",
    "    \n",
    "    Args:\n",
    "        md5_hex: MD5 hash as hex string (32 chars)\n",
    "        \n",
    "    Returns:\n",
    "        URIRef or None if invalid input\n",
    "        \n",
    "    Example:\n",
    "        md5 = \"a1b2c3d4e5f6789012345678901234ab\"\n",
    "        â†’ dca-id:file_a1b2c3d4e5f67890\n",
    "    \"\"\"\n",
    "    if not md5_hex or not isinstance(md5_hex, str):\n",
    "        return None\n",
    "    \n",
    "    # Clean and validate\n",
    "    md5_clean = md5_hex.strip().lower()\n",
    "    if len(md5_clean) < 16 or not re.match(r'^[a-f0-9]+$', md5_clean):\n",
    "        return None\n",
    "    \n",
    "    # Use first 16 characters for consistent short IDs\n",
    "    short_id = md5_clean[:16]\n",
    "    return URIRef(DCA_ID_BASE + f\"file_{short_id}\")\n",
    "\n",
    "def dca_file_uri_from_path_fallback(file_path: str) -> URIRef:\n",
    "    \"\"\"\n",
    "    Fallback ID generation from file path (when MD5 not available).\n",
    "    \n",
    "    WARNING: Only use this if DROID MD5 is unavailable!\n",
    "    Path-based IDs are less stable than content-based MD5.\n",
    "    \"\"\"\n",
    "    path_normalized = str(Path(file_path)).replace('\\\\', '/')  # Normalize separators\n",
    "    path_hash = hashlib.sha256(path_normalized.encode('utf-8')).hexdigest()[:16]\n",
    "    return URIRef(DCA_ID_BASE + f\"file_{path_hash}\")\n",
    "\n",
    "def dca_project_uri(project_name: str) -> URIRef:\n",
    "    \"\"\"\n",
    "    Generate project URI following DCA conventions.\n",
    "    \n",
    "    Example: \"WeingutGantenbein\" â†’ dca-id:project_WeingutGantenbein\n",
    "    \"\"\"\n",
    "    # Sanitize project name for URI use\n",
    "    sanitized = re.sub(r'[^a-zA-Z0-9_-]', '_', project_name)\n",
    "    return URIRef(DCA_ID_BASE + f\"project_{sanitized}\")\n",
    "\n",
    "def dca_activity_uri(activity_name: str) -> URIRef:\n",
    "    \"\"\"\n",
    "    Generate activity URI for provenance tracking.\n",
    "    \n",
    "    Example: \"ArchivingGantenbein2026\" â†’ dca-id:ArchivingGantenbein2026\n",
    "    \"\"\"\n",
    "    sanitized = re.sub(r'[^a-zA-Z0-9_-]', '_', activity_name)\n",
    "    return URIRef(DCA_ID_BASE + sanitized)\n",
    "\n",
    "# Test the ID generation\n",
    "test_md5 = \"a1b2c3d4e5f6789012345678901234ab\"\n",
    "test_uri = dca_file_uri_from_md5(test_md5)\n",
    "print(f\"âœ… ID Generation Functions Ready\")\n",
    "print(f\"ğŸ§ª Test MD5: {test_md5}\")\n",
    "print(f\"ğŸ†” Generated URI: {test_uri}\")\n",
    "print(f\"ğŸ“ ID Length: {len(str(test_uri).split('_')[1])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ab83d",
   "metadata": {},
   "source": [
    "## ğŸ“‚ Schritt 4: RenkuLab Pfad-Konfiguration\n",
    "\n",
    "**ANPASSEN**: Diese Pfade mÃ¼ssen an Ihre RenkuLab-Projektstruktur angepasst werden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3334401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# RENKULAB PATH CONFIGURATION\n",
    "# =====================================================\n",
    "# TODO: Adjust these paths to match your RenkuLab project structure!\n",
    "\n",
    "# Input: DROID CSV results\n",
    "DROID_CSV_PATH = Path(\"/home/renku/work/data/gramazio-kohler-archiv-server_DROIDresults.csv\")\n",
    "# Alternative if in project subdirectory:\n",
    "# DROID_CSV_PATH = Path(\"./data/droid_results.csv\")\n",
    "\n",
    "# Input: Base directory containing the actual files (for ExifTool)\n",
    "FILES_BASE_DIR = Path(\"/home/renku/work/data/gramazio-kohler-archiv-server/\")\n",
    "# Alternative:\n",
    "# FILES_BASE_DIR = Path(\"./data/files/\")\n",
    "\n",
    "# Output: Generated RDF file\n",
    "OUTPUT_RDF_PATH = Path(\"./dca_catalog_generated.ttl\")\n",
    "\n",
    "# Optional: Existing RDF to merge with\n",
    "EXISTING_RDF_PATH = Path(\"./existing_catalog.ttl\")  # Set to None if not available\n",
    "\n",
    "# ExifTool configuration\n",
    "EXIFTOOL_COMMAND = \"exiftool\"  # Adjust if installed in different location\n",
    "# For custom installation: \n",
    "# EXIFTOOL_COMMAND = \"/home/renku/work/tools/exiftool/exiftool\"\n",
    "\n",
    "# Project metadata\n",
    "PROJECT_NAME = \"WeingutGantenbein\"  # Will become dca-id:project_WeingutGantenbein\n",
    "ACTIVITY_NAME = \"ArchivingGantenbein2026\"  # Provenance activity\n",
    "\n",
    "# Validation\n",
    "print(\"ğŸ“‚ RenkuLab Path Configuration:\")\n",
    "print(f\"ğŸ“Š DROID CSV: {DROID_CSV_PATH}\")\n",
    "print(f\"ğŸ“ Files Base: {FILES_BASE_DIR}\")\n",
    "print(f\"ğŸ’¾ Output RDF: {OUTPUT_RDF_PATH}\")\n",
    "print(f\"ğŸ—ï¸  Project: {PROJECT_NAME}\")\n",
    "\n",
    "# Check critical paths\n",
    "if DROID_CSV_PATH.exists():\n",
    "    print(f\"âœ… DROID CSV found: {DROID_CSV_PATH.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "else:\n",
    "    print(f\"âŒ DROID CSV not found: {DROID_CSV_PATH}\")\n",
    "    print(\"   â†’ Please update DROID_CSV_PATH to correct location\")\n",
    "\n",
    "if FILES_BASE_DIR.exists():\n",
    "    print(f\"âœ… Files base directory found\")\n",
    "else:\n",
    "    print(f\"âŒ Files base directory not found: {FILES_BASE_DIR}\")\n",
    "    print(\"   â†’ ExifTool operations will be skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4c0c7",
   "metadata": {},
   "source": [
    "## ğŸ“Š Schritt 5: DROID CSV Laden & Analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# LOAD AND ANALYZE DROID CSV DATA\n",
    "# =====================================================\n",
    "\n",
    "def load_droid_csv(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load DROID CSV with proper encoding and error handling.\n",
    "    \n",
    "    Returns DataFrame with standardized column names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try UTF-8 first\n",
    "        df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            # Fallback to latin-1\n",
    "            df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "            print(\"âš ï¸  Used latin-1 encoding for CSV\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to read CSV: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    print(f\"ğŸ“Š DROID CSV loaded: {len(df):,} records\")\n",
    "    print(f\"ğŸ“‹ Columns: {list(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_droid_data(df: pd.DataFrame) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Analyze DROID data for DCA processing insights.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return {}\n",
    "    \n",
    "    analysis = {}\n",
    "    \n",
    "    # File type analysis\n",
    "    if 'EXT' in df.columns:\n",
    "        ext_counts = df['EXT'].value_counts()\n",
    "        analysis['extensions'] = ext_counts.head(20).to_dict()\n",
    "        \n",
    "        # Target extensions analysis\n",
    "        df['ext_lower'] = df['EXT'].str.lower() if 'EXT' in df.columns else ''\n",
    "        target_mask = df['ext_lower'].isin(TARGET_EXTENSIONS)\n",
    "        analysis['target_files'] = target_mask.sum()\n",
    "        analysis['total_files'] = len(df)\n",
    "    \n",
    "    # MD5 hash analysis\n",
    "    md5_col = None\n",
    "    for col in ['MD5_HASH', 'HASH', 'md5', 'MD5']:\n",
    "        if col in df.columns:\n",
    "            md5_col = col\n",
    "            break\n",
    "    \n",
    "    if md5_col:\n",
    "        analysis['md5_column'] = md5_col\n",
    "        analysis['md5_available'] = df[md5_col].notna().sum()\n",
    "    else:\n",
    "        analysis['md5_column'] = None\n",
    "        analysis['md5_available'] = 0\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Load the data\n",
    "print(\"ğŸ”„ Loading DROID CSV data...\")\n",
    "droid_df = load_droid_csv(DROID_CSV_PATH)\n",
    "\n",
    "if not droid_df.empty:\n",
    "    analysis = analyze_droid_data(droid_df)\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ DROID Data Analysis:\")\n",
    "    print(f\"ğŸ“Š Total files: {analysis.get('total_files', 0):,}\")\n",
    "    print(f\"ğŸ¯ Target files (img/adobe/cad): {analysis.get('target_files', 0):,}\")\n",
    "    print(f\"#ï¸âƒ£  MD5 hashes available: {analysis.get('md5_available', 0):,}\")\n",
    "    if analysis.get('md5_column'):\n",
    "        print(f\"ğŸ”‘ MD5 column: '{analysis['md5_column']}'\")\n",
    "    \n",
    "    if analysis.get('extensions'):\n",
    "        print(\"\\nğŸ“ Top file extensions:\")\n",
    "        for ext, count in list(analysis['extensions'].items())[:10]:\n",
    "            print(f\"   .{ext}: {count:,} files\")\n",
    "else:\n",
    "    print(\"âŒ No DROID data available - check file path!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe2fb7",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Schritt 6: RDF Graph Initialisierung mit DCA Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17efc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# INITIALIZE RDF GRAPH WITH DCA ONTOLOGY\n",
    "# =====================================================\n",
    "\n",
    "def create_dca_graph() -> Graph:\n",
    "    \"\"\"\n",
    "    Create RDF graph with DCA ontology structure and namespace bindings.\n",
    "    \"\"\"\n",
    "    g = Graph()\n",
    "    \n",
    "    # Bind namespaces for clean Turtle output\n",
    "    g.bind(\"dca\", DCA)\n",
    "    g.bind(\"dca-id\", DCA_ID)\n",
    "    g.bind(\"dca-tech\", DCA_TECH)\n",
    "    g.bind(\"premis\", PREMIS)\n",
    "    g.bind(\"rico\", RICO)\n",
    "    g.bind(\"dcterms\", DCTERMS)\n",
    "    g.bind(\"owl\", OWL)\n",
    "    g.bind(\"xsd\", XSD)\n",
    "    \n",
    "    return g\n",
    "\n",
    "def add_ontology_definitions(g: Graph):\n",
    "    \"\"\"\n",
    "    Add DCA ontology class and property definitions.\n",
    "    \"\"\"\n",
    "    # Ontology declaration\n",
    "    ontology_uri = URIRef(\"http://dca.ethz.ch/ontology\")\n",
    "    g.add((ontology_uri, RDF.type, OWL.Ontology))\n",
    "    g.add((ontology_uri, DCTERMS.created, Literal(datetime.now().strftime(\"%Y-%m-%d\"), datatype=XSD.date)))\n",
    "    g.add((ontology_uri, DCTERMS.creator, Literal(\"ETH Zurich - Digital Construction Archive Project\")))\n",
    "    g.add((ontology_uri, DCTERMS.description, \n",
    "           Literal(\"Standards-based ontology for digital construction archives using RiC-O, PREMIS, and Dublin Core with clean namespace separation and Nextcloud WebDAV integration\", lang=\"en\")))\n",
    "    \n",
    "    # DCA Classes\n",
    "    g.add((DCA.ConstructionProject, RDF.type, OWL.Class))\n",
    "    g.add((DCA.ConstructionProject, RDFS.label, Literal(\"Construction Project\", lang=\"en\")))\n",
    "    g.add((DCA.ConstructionProject, RDFS.comment, \n",
    "           Literal(\"A construction or architectural project containing digital files\", lang=\"en\")))\n",
    "    g.add((DCA.ConstructionProject, RDFS.subClassOf, RICO.RecordSet))\n",
    "    \n",
    "    g.add((DCA.ArchiveFile, RDF.type, OWL.Class))\n",
    "    g.add((DCA.ArchiveFile, RDFS.label, Literal(\"Archive File\", lang=\"en\")))\n",
    "    g.add((DCA.ArchiveFile, RDFS.comment, \n",
    "           Literal(\"A digital file within the construction archive\", lang=\"en\")))\n",
    "    g.add((DCA.ArchiveFile, RDFS.subClassOf, PREMIS.Object))\n",
    "    g.add((DCA.ArchiveFile, RDFS.subClassOf, RICO.Record))\n",
    "    \n",
    "    # DCA Properties\n",
    "    g.add((DCA.belongsToProject, RDF.type, OWL.ObjectProperty))\n",
    "    g.add((DCA.belongsToProject, RDFS.label, Literal(\"belongs to project\", lang=\"en\")))\n",
    "    g.add((DCA.belongsToProject, RDFS.comment, \n",
    "           Literal(\"Indicates that a file belongs to a specific construction project\", lang=\"en\")))\n",
    "    g.add((DCA.belongsToProject, RDFS.domain, DCA.ArchiveFile))\n",
    "    g.add((DCA.belongsToProject, RDFS.range, DCA.ConstructionProject))\n",
    "    \n",
    "    # Technical properties\n",
    "    g.add((DCA_TECH.sourcePath, RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((DCA_TECH.sourcePath, RDFS.label, Literal(\"source path\", lang=\"en\")))\n",
    "    g.add((DCA_TECH.sourcePath, RDFS.comment, \n",
    "           Literal(\"DEPRECATED: Original filesystem path from DROID analysis. Use dcterms:identifier with Nextcloud WebDAV URL instead to avoid redundancy.\", lang=\"en\")))\n",
    "    g.add((DCA_TECH.sourcePath, RDFS.domain, RICO.Record))\n",
    "    g.add((DCA_TECH.sourcePath, RDFS.range, XSD.string))\n",
    "    g.add((DCA_TECH.sourcePath, OWL.deprecated, Literal(True)))\n",
    "\n",
    "def add_project_and_activity(g: Graph, project_name: str, activity_name: str):\n",
    "    \"\"\"\n",
    "    Add project and provenance activity to graph.\n",
    "    \"\"\"\n",
    "    # Project\n",
    "    project_uri = dca_project_uri(project_name)\n",
    "    g.add((project_uri, RDF.type, DCA.ConstructionProject))\n",
    "    g.add((project_uri, RDF.type, RICO.RecordSet))\n",
    "    g.add((project_uri, DCTERMS.title, Literal(project_name)))\n",
    "    \n",
    "    # Activity\n",
    "    activity_uri = dca_activity_uri(activity_name)\n",
    "    g.add((activity_uri, RDF.type, RICO.Activity))\n",
    "    g.add((activity_uri, RDFS.label, Literal(f\"Digitale Archivierung {project_name} Konstruktionsunterlagen\")))\n",
    "    g.add((activity_uri, DCTERMS.description, \n",
    "           Literal(f\"Systematische digitale Erfassung und Archivierung der Konstruktionsdokumentation des {project_name} Projekts. EnthÃ¤lt PlÃ¤ne, Fotos, technische Berichte und Korrespondenz.\")))\n",
    "    g.add((activity_uri, RICO.hasEndDate, Literal(\"Februar 2026\")))\n",
    "    g.add((activity_uri, RICO.occurredAtDate, Literal(datetime.now().isoformat(), datatype=XSD.dateTime)))\n",
    "    g.add((activity_uri, RICO.technique, \n",
    "           Literal(\"DROID file identification with PRONOM registry, RDF metadata generation, Nextcloud WebDAV storage\")))\n",
    "    \n",
    "    # Team\n",
    "    team_uri = URIRef(DCA_ID_BASE + \"DCA_Team\")\n",
    "    g.add((team_uri, RDF.type, RICO.Group))\n",
    "    g.add((team_uri, RDFS.label, Literal(\"Digital Construction Archive Team, ETH ZÃ¼rich\")))\n",
    "    g.add((activity_uri, RICO.isOrWasPerformedBy, team_uri))\n",
    "    \n",
    "    # Link project to activity\n",
    "    g.add((project_uri, RICO.isOrWasDocumentedBy, activity_uri))\n",
    "    \n",
    "    return project_uri, activity_uri\n",
    "\n",
    "# Initialize the graph\n",
    "print(\"ğŸ”„ Initializing DCA RDF Graph...\")\n",
    "graph = create_dca_graph()\n",
    "\n",
    "# Add ontology structure\n",
    "add_ontology_definitions(graph)\n",
    "print(\"âœ… DCA ontology definitions added\")\n",
    "\n",
    "# Add project and activity\n",
    "project_uri, activity_uri = add_project_and_activity(graph, PROJECT_NAME, ACTIVITY_NAME)\n",
    "print(f\"âœ… Project added: {project_uri}\")\n",
    "print(f\"âœ… Activity added: {activity_uri}\")\n",
    "\n",
    "print(f\"ğŸ“Š Current graph size: {len(graph)} triples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e6daf5",
   "metadata": {},
   "source": [
    "## ğŸ”„ Schritt 7: DROID Daten zu RDF Konvertierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794fcddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# CONVERT DROID DATA TO RDF\n",
    "# =====================================================\n",
    "\n",
    "def safe_literal(value, datatype=None, lang=None):\n",
    "    \"\"\"\n",
    "    Create RDF literal with error handling.\n",
    "    \"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return None\n",
    "    try:\n",
    "        return Literal(str(value), datatype=datatype, lang=lang)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Literal creation failed for '{value}': {e}\")\n",
    "        return Literal(str(value))  # Fallback without datatype\n",
    "\n",
    "def add_premis_identifier(g: Graph, file_uri: URIRef, id_type: str, value: str):\n",
    "    \"\"\"\n",
    "    Add PREMIS identifier as blank node.\n",
    "    \"\"\"\n",
    "    if not value or pd.isna(value):\n",
    "        return\n",
    "    \n",
    "    bn = BNode()\n",
    "    g.add((file_uri, PREMIS.hasIdentifier, bn))\n",
    "    g.add((bn, PREMIS.identifierType, Literal(id_type)))\n",
    "    g.add((bn, PREMIS.identifierValue, Literal(str(value))))\n",
    "\n",
    "def process_droid_record(g: Graph, record: pd.Series, project_uri: URIRef) -> Optional[URIRef]:\n",
    "    \"\"\"\n",
    "    Process single DROID record and add to RDF graph.\n",
    "    \n",
    "    Returns the created file URI or None if processing failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine MD5 hash column (DROID CSV variations)\n",
    "        md5_hash = None\n",
    "        for col in ['MD5_HASH', 'HASH', 'md5', 'MD5']:\n",
    "            if col in record.index and not pd.isna(record.get(col)):\n",
    "                md5_hash = record[col]\n",
    "                break\n",
    "        \n",
    "        # Generate file URI\n",
    "        if md5_hash:\n",
    "            file_uri = dca_file_uri_from_md5(md5_hash)\n",
    "        else:\n",
    "            # Fallback to path-based ID\n",
    "            file_path = record.get('FILE_PATH', record.get('PATH', ''))\n",
    "            if not file_path:\n",
    "                return None\n",
    "            file_uri = dca_file_uri_from_path_fallback(file_path)\n",
    "        \n",
    "        if not file_uri:\n",
    "            return None\n",
    "        \n",
    "        # Core classes\n",
    "        g.add((file_uri, RDF.type, DCA.ArchiveFile))\n",
    "        g.add((file_uri, RDF.type, PREMIS.Object))\n",
    "        g.add((file_uri, RDF.type, RICO.Record))\n",
    "        \n",
    "        # Project relationship\n",
    "        g.add((file_uri, DCA.belongsToProject, project_uri))\n",
    "        g.add((file_uri, RICO.isOrWasIncludedIn, project_uri))\n",
    "        \n",
    "        # Basic metadata\n",
    "        if 'NAME' in record.index:\n",
    "            g.add((file_uri, DCTERMS.title, safe_literal(record['NAME'])))\n",
    "        \n",
    "        # File path as identifier (WebDAV-style if possible)\n",
    "        file_path = record.get('FILE_PATH', record.get('PATH', ''))\n",
    "        if file_path:\n",
    "            # Convert to WebDAV URL format if possible\n",
    "            webdav_url = f\"https://nextcloud.ethz.ch/remote.php/dav/files/padrian/DCA/{PROJECT_NAME}/{file_path}\"\n",
    "            g.add((file_uri, DCTERMS.identifier, URIRef(webdav_url)))\n",
    "        \n",
    "        # Timestamps\n",
    "        for time_col in ['LAST_MODIFIED', 'MODIFIED', 'DATE_MODIFIED']:\n",
    "            if time_col in record.index and not pd.isna(record[time_col]):\n",
    "                timestamp = safe_literal(record[time_col], datatype=XSD.dateTime)\n",
    "                if timestamp:\n",
    "                    g.add((file_uri, DCTERMS.modified, timestamp))\n",
    "                break\n",
    "        \n",
    "        # PREMIS format information\n",
    "        if 'FORMAT_NAME' in record.index:\n",
    "            g.add((file_uri, PREMIS.hasFormatName, safe_literal(record['FORMAT_NAME'])))\n",
    "        \n",
    "        # Format details\n",
    "        format_notes = []\n",
    "        if 'MIME_TYPE' in record.index and not pd.isna(record['MIME_TYPE']):\n",
    "            format_notes.append(f\"MIME: {record['MIME_TYPE']}\")\n",
    "        if 'PUID' in record.index and not pd.isna(record['PUID']):\n",
    "            format_notes.append(f\"PRONOM ID: {record['PUID']}\")\n",
    "        \n",
    "        for note in format_notes:\n",
    "            g.add((file_uri, PREMIS.hasFormatNote, Literal(note)))\n",
    "        \n",
    "        # File size\n",
    "        if 'SIZE' in record.index and not pd.isna(record['SIZE']):\n",
    "            try:\n",
    "                size_val = int(float(record['SIZE']))\n",
    "                g.add((file_uri, PREMIS.hasSize, Literal(size_val, datatype=XSD.long)))\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "        \n",
    "        # DROID identification method\n",
    "        g.add((file_uri, PREMIS.hasCreatingApplication, Literal(\"DROID: Signature\")))\n",
    "        \n",
    "        return file_uri\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Failed to process record: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process DROID data\n",
    "if not droid_df.empty:\n",
    "    print(f\"ğŸ”„ Processing {len(droid_df):,} DROID records...\")\n",
    "    \n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for idx, record in droid_df.iterrows():\n",
    "        file_uri = process_droid_record(graph, record, project_uri)\n",
    "        if file_uri:\n",
    "            processed_count += 1\n",
    "        else:\n",
    "            error_count += 1\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (idx + 1) % 1000 == 0:\n",
    "            print(f\"   Processed: {idx + 1:,} / {len(droid_df):,} records\")\n",
    "    \n",
    "    print(f\"âœ… DROID processing complete:\")\n",
    "    print(f\"   ğŸ“Š Successfully processed: {processed_count:,} files\")\n",
    "    print(f\"   âŒ Errors: {error_count:,} files\")\n",
    "    print(f\"   ğŸ“ˆ Graph size: {len(graph):,} triples\")\n",
    "else:\n",
    "    print(\"âŒ No DROID data to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1ea5b1",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Schritt 8: RDF Export & Validierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# EXPORT RDF AND VALIDATE\n",
    "# =====================================================\n",
    "\n",
    "def export_rdf_turtle(g: Graph, output_path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Export RDF graph to Turtle format with proper formatting.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Serialize to Turtle with nice formatting\n",
    "        turtle_data = g.serialize(format='turtle')\n",
    "        \n",
    "        # Write to file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(turtle_data)\n",
    "        \n",
    "        file_size = output_path.stat().st_size\n",
    "        print(f\"âœ… RDF exported to: {output_path}\")\n",
    "        print(f\"ğŸ“Š File size: {file_size / 1024 / 1024:.2f} MB\")\n",
    "        print(f\"ğŸ“ˆ Triples: {len(g):,}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Export failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def validate_rdf_content(g: Graph) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Validate RDF content and provide statistics.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # Count by type\n",
    "    stats['archive_files'] = len(list(g.subjects(RDF.type, DCA.ArchiveFile)))\n",
    "    stats['projects'] = len(list(g.subjects(RDF.type, DCA.ConstructionProject)))\n",
    "    stats['activities'] = len(list(g.subjects(RDF.type, RICO.Activity)))\n",
    "    \n",
    "    # Count properties\n",
    "    stats['has_title'] = len(list(g.subjects(DCTERMS.title, None)))\n",
    "    stats['has_identifier'] = len(list(g.subjects(DCTERMS.identifier, None)))\n",
    "    stats['has_format'] = len(list(g.subjects(PREMIS.hasFormatName, None)))\n",
    "    stats['has_size'] = len(list(g.subjects(PREMIS.hasSize, None)))\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Export RDF\n",
    "print(\"ğŸ’¾ Exporting RDF to Turtle format...\")\n",
    "export_success = export_rdf_turtle(graph, OUTPUT_RDF_PATH)\n",
    "\n",
    "# Validate content\n",
    "if export_success:\n",
    "    print(\"\\nğŸ” RDF Content Validation:\")\n",
    "    stats = validate_rdf_content(graph)\n",
    "    \n",
    "    for key, value in stats.items():\n",
    "        print(f\"   {key.replace('_', ' ').title()}: {value:,}\")\n",
    "    \n",
    "    # Quality checks\n",
    "    if stats.get('archive_files', 0) > 0:\n",
    "        coverage_title = (stats.get('has_title', 0) / stats['archive_files']) * 100\n",
    "        coverage_format = (stats.get('has_format', 0) / stats['archive_files']) * 100\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Metadata Coverage:\")\n",
    "        print(f\"   Titles: {coverage_title:.1f}%\")\n",
    "        print(f\"   Format info: {coverage_format:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nâœ… RDF generation complete!\")\n",
    "    print(f\"ğŸ“‚ Output file: {OUTPUT_RDF_PATH.absolute()}\")\n",
    "else:\n",
    "    print(\"âŒ RDF export failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251bd17",
   "metadata": {},
   "source": [
    "## ğŸ¯ Schritt 9: Beispiel-Abfragen zur Verifikation\n",
    "\n",
    "Diese Queries helfen dabei, die generierte RDF zu validieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a9a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# SAMPLE SPARQL QUERIES FOR VALIDATION\n",
    "# =====================================================\n",
    "\n",
    "def run_sample_queries(g: Graph):\n",
    "    \"\"\"\n",
    "    Run sample SPARQL queries to validate RDF structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    queries = {\n",
    "        \"Total Files\": \"\"\"\n",
    "            SELECT (COUNT(?file) AS ?count) WHERE {\n",
    "                ?file a dca:ArchiveFile .\n",
    "            }\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Image Files\": \"\"\"\n",
    "            SELECT ?file ?title ?format WHERE {\n",
    "                ?file a dca:ArchiveFile ;\n",
    "                      dcterms:title ?title ;\n",
    "                      premis:hasFormatName ?format .\n",
    "                FILTER(CONTAINS(LCASE(?format), \"image\") || \n",
    "                       CONTAINS(LCASE(?format), \"jpeg\") || \n",
    "                       CONTAINS(LCASE(?format), \"tiff\"))\n",
    "            } LIMIT 10\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Project Info\": \"\"\"\n",
    "            SELECT ?project ?title ?fileCount WHERE {\n",
    "                ?project a dca:ConstructionProject ;\n",
    "                         dcterms:title ?title .\n",
    "                {\n",
    "                    SELECT ?project (COUNT(?file) AS ?fileCount) WHERE {\n",
    "                        ?file dca:belongsToProject ?project .\n",
    "                    } GROUP BY ?project\n",
    "                }\n",
    "            }\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Format Distribution\": \"\"\"\n",
    "            SELECT ?format (COUNT(?file) AS ?count) WHERE {\n",
    "                ?file a dca:ArchiveFile ;\n",
    "                      premis:hasFormatName ?format .\n",
    "            } GROUP BY ?format ORDER BY DESC(?count) LIMIT 10\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ” Running validation queries...\\n\")\n",
    "    \n",
    "    for query_name, query_text in queries.items():\n",
    "        try:\n",
    "            print(f\"ğŸ“Š {query_name}:\")\n",
    "            results = g.query(query_text)\n",
    "            \n",
    "            for row in results:\n",
    "                values = [str(val) for val in row]\n",
    "                print(f\"   {' | '.join(values)}\")\n",
    "            \n",
    "            print(f\"   ({len(results)} results)\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Query failed: {e}\\n\")\n",
    "\n",
    "# Run validation queries\n",
    "if len(graph) > 0:\n",
    "    run_sample_queries(graph)\n",
    "else:\n",
    "    print(\"âš ï¸  No data in graph to query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640bdb3b",
   "metadata": {},
   "source": [
    "## âœ… Pipeline Abschluss & NÃ¤chste Schritte\n",
    "\n",
    "Die CSV-zu-RDF Pipeline ist abgeschlossen. Die generierte RDF-Datei kann nun:\n",
    "\n",
    "### ğŸ“¤ ZurÃ¼ck nach ETH DCA Ã¼bertragen werden\n",
    "- Upload der `.ttl` Datei\n",
    "- Merge mit bestehenden RDF-Daten\n",
    "- Integration in Triple Store (Apache Jena Fuseki)\n",
    "\n",
    "### ğŸ”„ FÃ¼r weitere Verarbeitung verwendet werden\n",
    "- XMP-Metadaten Extraktion mit ExifTool\n",
    "- Provenience-Daten aus Adobe Creative Suite\n",
    "- Verlinkung zwischen verwandten Dateien\n",
    "\n",
    "### ğŸ“Š FÃ¼r Analysen und Visualisierungen\n",
    "- SPARQL-Queries fÃ¼r Statistiken\n",
    "- Netzwerk-Analysen der Datei-Dependencies\n",
    "- Timeline-Visualisierungen der Projektentwicklung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41109e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# PIPELINE SUMMARY\n",
    "# =====================================================\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f\"ğŸ DCA CSV-zu-RDF Pipeline abgeschlossen\")\n",
    "print(f\"ğŸ“… Ende: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ“Š Finale Graph-GrÃ¶ÃŸe: {len(graph):,} triples\")\n",
    "print(f\"ğŸ“„ Output-Datei: {OUTPUT_RDF_PATH.absolute()}\")\n",
    "\n",
    "if OUTPUT_RDF_PATH.exists():\n",
    "    file_size_mb = OUTPUT_RDF_PATH.stat().st_size / 1024 / 1024\n",
    "    print(f\"ğŸ“ DateigrÃ¶ÃŸe: {file_size_mb:.2f} MB\")\n",
    "    print(f\"âœ… Bereit fÃ¼r Transfer zu ETH DCA System\")\n",
    "else:\n",
    "    print(f\"âŒ Output-Datei wurde nicht erstellt\")\n",
    "\n",
    "print(\"\\nğŸ”„ NÃ¤chste Schritte:\")\n",
    "print(\"   1. RDF-Datei nach ETH DCA System Ã¼bertragen\")\n",
    "print(\"   2. Mit bestehenden Daten mergen\")\n",
    "print(\"   3. ExifTool fÃ¼r XMP-Metadaten einsetzen\")\n",
    "print(\"   4. SPARQL-Endpoint fÃ¼r Abfragen einrichten\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
