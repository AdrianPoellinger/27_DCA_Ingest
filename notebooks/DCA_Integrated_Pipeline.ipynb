{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d480d780",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Digital Construction Archive (DCA) - Integrierte Pipeline\n",
    "\n",
    "**Zweck**: Vollst√§ndige Integration von DROID Analyse ‚Üí RDF Konvertierung ‚Üí EXIF/XMP Anreicherung  \n",
    "**Zielumgebung**: Lokale Entwicklung mit ETH DCA Standards  \n",
    "**Standards**: RiC-O, PREMIS, Dublin Core mit DCA-spezifischen Erweiterungen  \n",
    "**Datum**: M√§rz 2026  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Pipeline-√úbersicht\n",
    "\n",
    "Diese Pipeline f√ºhrt drei kritische Schritte in einem integrierten Workflow durch:\n",
    "\n",
    "1. **üîç DROID File Analysis**: Systematische Dateierkennung mit MD5-Hashes\n",
    "2. **üîó RDF Conversion**: DROID CSV ‚Üí standardkonforme RDF mit DCA Ontologie  \n",
    "3. **üì∏ XMP Integration**: ExifTool XMP-Metadaten ‚Üí PREMIS Identifier & Derivations\n",
    "\n",
    "### ‚úÖ Validierung zwischen jedem Schritt\n",
    "- Dateivollst√§ndigkeit pr√ºfen\n",
    "- Konsistenz der MD5-basierten URIs sicherstellen  \n",
    "- Alle relevanten Dateien erfassen und verarbeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171cbfdc",
   "metadata": {},
   "source": [
    "## üì¶ 1. Environment Setup & Dependencies\n",
    "\n",
    "Installation und Import aller erforderlichen Bibliotheken mit Logging f√ºr Prozess-Tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ad6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# ENVIRONMENT SETUP & DEPENDENCIES\n",
    "# =====================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json, subprocess, hashlib, sys, math, re, os, shutil\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, List, Set, Union\n",
    "import warnings\n",
    "import logging\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# RDF Core Libraries\n",
    "from rdflib import Graph, Namespace, URIRef, BNode, Literal\n",
    "from rdflib.namespace import RDF, RDFS, XSD, DCTERMS\n",
    "from rdflib.plugins.serializers.turtle import TurtleSerializer\n",
    "\n",
    "# Optional: Network analysis for provenance graphs\n",
    "try:\n",
    "    import networkx as nx\n",
    "    NX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  NetworkX not available - provenance graphs disabled\")\n",
    "    NX_AVAILABLE = False\n",
    "\n",
    "# Logging Setup f√ºr Pipeline-Tracking\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler(f'dca_pipeline_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('DCA_Pipeline')\n",
    "\n",
    "# Pipeline Status Tracking\n",
    "pipeline_status = {\n",
    "    'start_time': datetime.now(),\n",
    "    'steps_completed': [],\n",
    "    'steps_failed': [],\n",
    "    'file_counts': {},\n",
    "    'errors': []\n",
    "}\n",
    "\n",
    "def log_step(step_name: str, success: bool, details: str = \"\"):\n",
    "    \"\"\"Log pipeline step with status tracking\"\"\"\n",
    "    timestamp = datetime.now().strftime('%H:%M:%S')\n",
    "    if success:\n",
    "        pipeline_status['steps_completed'].append(step_name)\n",
    "        logger.info(f\"‚úÖ [{timestamp}] {step_name}: {details}\")\n",
    "    else:\n",
    "        pipeline_status['steps_failed'].append(step_name)\n",
    "        logger.error(f\"‚ùå [{timestamp}] {step_name}: {details}\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All dependencies loaded successfully\")\n",
    "print(f\"üìÖ DCA Integrated Pipeline started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üîß Python {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(f\"üìö RDFLib version: {getattr(Graph(), 'version', 'unknown')}\")\n",
    "print(f\"üìã Log file: dca_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
    "\n",
    "log_step(\"Environment Setup\", True, \"All dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f907a0",
   "metadata": {},
   "source": [
    "## üìÇ 2. Path Configuration & Validation\n",
    "\n",
    "Setup dynamischer Pfade f√ºr DROID Binary, Input-Ordner und Output-Verzeichnisse mit Validierung aller erforderlichen Dateien und Verzeichnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# PATH CONFIGURATION & VALIDATION\n",
    "# =====================================================\n",
    "\n",
    "# Project Configuration (ANPASSEN f√ºr verschiedene Projekte)\n",
    "project_path = \"WeingutGantenbein\"\n",
    "dataset_to_analyze = \"gramazio-kohler-archiv-server\"\n",
    "\n",
    "# Dynamisches Home-Verzeichnis (lokale Entwicklung)\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "base_path = os.path.join(home_dir, \"work\")\n",
    "\n",
    "# DROID Configuration\n",
    "droid_script_path = os.path.join(base_path, \"27_DCA_Ingest/src/droid-binary-6.7.0-bin/droid.sh\")\n",
    "folder_to_analyze = os.path.join(base_path, f\"dcaonnextcloud-500gb/DigitalMaterialCopies/{project_path}/{dataset_to_analyze}\")\n",
    "output_folder = os.path.join(base_path, f\"dcaonnextcloud-500gb/dca-metadataraw/{project_path}/{dataset_to_analyze}_results\")\n",
    "droid_csv_path = os.path.join(output_folder, f\"{dataset_to_analyze}_DROIDresults.csv\")\n",
    "\n",
    "# RDF Output Configuration\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "rdf_output_path = Path(output_folder) / f\"{dataset_to_analyze}_catalog_integrated_{timestamp}.ttl\"\n",
    "rdf_backup_path = Path(output_folder) / f\"{dataset_to_analyze}_backup_{timestamp}.ttl\"\n",
    "\n",
    "# ExifTool Configuration\n",
    "exiftool_command = \"/opt/homebrew/bin/exiftool\"  # Adjust for your system\n",
    "\n",
    "# Files base directory for XMP processing\n",
    "files_base_dir = Path(folder_to_analyze)\n",
    "\n",
    "# Project metadata for RDF\n",
    "PROJECT_NAME = project_path\n",
    "ACTIVITY_NAME = f\"IntegratedArchiving{project_path}2026\"\n",
    "\n",
    "# Path zu src/ hinzuf√ºgen, um lokale Module zu importieren\n",
    "src_path = os.path.join(base_path, \"27_DCA_Ingest/src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# =====================================================\n",
    "# PATH VALIDATION\n",
    "# =====================================================\n",
    "\n",
    "def validate_paths():\n",
    "    \"\"\"Validate all required paths and files exist\"\"\"\n",
    "    validation_results = []\n",
    "    \n",
    "    # Check DROID script\n",
    "    if os.path.exists(droid_script_path):\n",
    "        validation_results.append((\"‚úÖ DROID script\", droid_script_path))\n",
    "    else:\n",
    "        validation_results.append((\"‚ùå DROID script\", droid_script_path))\n",
    "        pipeline_status['errors'].append(f\"DROID script not found: {droid_script_path}\")\n",
    "    \n",
    "    # Check folder to analyze\n",
    "    if os.path.exists(folder_to_analyze):\n",
    "        file_count = sum(1 for p in Path(folder_to_analyze).rglob(\"*\") if p.is_file())\n",
    "        validation_results.append((\"‚úÖ Input folder\", f\"{folder_to_analyze} ({file_count:,} files)\"))\n",
    "        pipeline_status['file_counts']['input_files'] = file_count\n",
    "    else:\n",
    "        validation_results.append((\"‚ùå Input folder\", folder_to_analyze))\n",
    "        pipeline_status['errors'].append(f\"Input folder not found: {folder_to_analyze}\")\n",
    "    \n",
    "    # Check/create output folder\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    if os.path.exists(output_folder):\n",
    "        validation_results.append((\"‚úÖ Output folder\", output_folder))\n",
    "    else:\n",
    "        validation_results.append((\"‚ùå Output folder\", output_folder))\n",
    "        pipeline_status['errors'].append(f\"Cannot create output folder: {output_folder}\")\n",
    "    \n",
    "    # Check ExifTool\n",
    "    try:\n",
    "        result = subprocess.run([exiftool_command, \"-ver\"], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            validation_results.append((\"‚úÖ ExifTool\", f\"version {result.stdout.strip()}\"))\n",
    "        else:\n",
    "            validation_results.append((\"‚ùå ExifTool\", \"not working\"))\n",
    "            pipeline_status['errors'].append(\"ExifTool not working\")\n",
    "    except FileNotFoundError:\n",
    "        validation_results.append((\"‚ùå ExifTool\", \"not found\"))\n",
    "        pipeline_status['errors'].append(f\"ExifTool not found: {exiftool_command}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "print(\"üìÇ PATH CONFIGURATION:\")\n",
    "print(f\"üèóÔ∏è  Projekt: {PROJECT_NAME}\")\n",
    "print(f\"üìä Dataset: {dataset_to_analyze}\")\n",
    "print(f\"üìÅ Input: {folder_to_analyze}\")\n",
    "print(f\"üíæ Output: {output_folder}\")\n",
    "print(f\"üìÑ RDF Output: {rdf_output_path}\")\n",
    "print()\n",
    "\n",
    "# Run validation\n",
    "validation_results = validate_paths()\n",
    "print(\"üîç PATH VALIDATION:\")\n",
    "for status, path in validation_results:\n",
    "    print(f\"   {status}: {path}\")\n",
    "\n",
    "# Check if all critical paths are valid\n",
    "critical_errors = [error for error in pipeline_status['errors'] if \"not found\" in error]\n",
    "if critical_errors:\n",
    "    print(\"\\n‚ùå CRITICAL ERRORS FOUND:\")\n",
    "    for error in critical_errors:\n",
    "        print(f\"   ‚Ä¢ {error}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Please fix path configuration before proceeding!\")\n",
    "    log_step(\"Path Validation\", False, f\"{len(critical_errors)} critical errors\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All paths validated successfully!\")\n",
    "    log_step(\"Path Validation\", True, \"All required paths found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cb3d64",
   "metadata": {},
   "source": [
    "## üîç 3. DROID File Analysis Execution\n",
    "\n",
    "Ausf√ºhrung der DROID Binary mit Hash-Generierung und Filteroptionen, Erfassung von Output und Fehlern, sowie ordnungsgem√§√üe Behandlung von Subprocess-Exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# DROID FILE ANALYSIS EXECUTION\n",
    "# =====================================================\n",
    "\n",
    "def run_droid_analysis():\n",
    "    \"\"\"Execute DROID analysis with comprehensive error handling\"\"\"\n",
    "    \n",
    "    # Skip if critical errors from path validation\n",
    "    if pipeline_status['errors']:\n",
    "        print(\"‚è≠Ô∏è  Skipping DROID analysis due to path validation errors\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"üîç Starting DROID Analysis...\")\n",
    "    print(f\"üìÅ Analyzing folder: {folder_to_analyze}\")\n",
    "    print(f\"üíæ Output will be saved to: {droid_csv_path}\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # DROID command with comprehensive options\n",
    "        droid_command = [\n",
    "            droid_script_path,\n",
    "            \"-R\", folder_to_analyze,           # Recursive analysis\n",
    "            \"-o\", droid_csv_path,              # Output CSV path\n",
    "            \"-Pr\", \"profile.generateHash=true\", # Generate MD5 hashes\n",
    "            \"-ff\", \"file_name not startswith ~$\"  # Filter temp files\n",
    "        ]\n",
    "        \n",
    "        print(f\"üöÄ Executing DROID command:\")\n",
    "        print(f\"   {' '.join(droid_command)}\")\n",
    "        \n",
    "        # Execute DROID with progress tracking\n",
    "        result = subprocess.run(\n",
    "            droid_command,\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        execution_time = datetime.now() - start_time\n",
    "        \n",
    "        # Process result\n",
    "        print(f\"‚úÖ DROID execution completed in {execution_time}\")\n",
    "        if result.stdout:\n",
    "            print(f\"üìù DROID output: {result.stdout}\")\n",
    "        \n",
    "        # Validate output file\n",
    "        if os.path.exists(droid_csv_path):\n",
    "            file_size = os.path.getsize(droid_csv_path) / 1024 / 1024\n",
    "            print(f\"üìÑ CSV file created: {file_size:.2f} MB\")\n",
    "            log_step(\"DROID Analysis\", True, f\"CSV generated: {file_size:.2f} MB\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Expected CSV file not created: {droid_csv_path}\")\n",
    "            log_step(\"DROID Analysis\", False, \"CSV file not created\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        error_msg = f\"DROID execution failed with exit code {e.returncode}\"\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        if e.stderr:\n",
    "            print(f\"üîç DROID error output: {e.stderr}\")\n",
    "        log_step(\"DROID Analysis\", False, f\"{error_msg}: {e.stderr}\")\n",
    "        pipeline_status['errors'].append(error_msg)\n",
    "        return False\n",
    "        \n",
    "    except PermissionError as e:\n",
    "        error_msg = f\"Permission error: {e}\"\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        print(\"   ‚Üí Pr√ºfe Schreibrechte f√ºr Zielordner\")\n",
    "        log_step(\"DROID Analysis\", False, error_msg)\n",
    "        pipeline_status['errors'].append(error_msg)\n",
    "        return False\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        error_msg = f\"File or directory not found: {e}\"\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        print(\"   ‚Üí Pr√ºfe alle Pfade auf Korrektheit\")\n",
    "        log_step(\"DROID Analysis\", False, error_msg)\n",
    "        pipeline_status['errors'].append(error_msg)\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Unexpected error during DROID analysis: {e}\"\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        log_step(\"DROID Analysis\", False, error_msg)\n",
    "        pipeline_status['errors'].append(error_msg)\n",
    "        return False\n",
    "\n",
    "# Execute DROID Analysis\n",
    "droid_success = run_droid_analysis()\n",
    "\n",
    "# Store result in pipeline status\n",
    "pipeline_status['droid_success'] = droid_success\n",
    "if droid_success:\n",
    "    pipeline_status['droid_csv_path'] = droid_csv_path\n",
    "    print(f\"\\nüéØ DROID Analysis completed successfully!\")\n",
    "    print(f\"   üìÑ Results available at: {droid_csv_path}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  DROID Analysis failed - check errors above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f894c1",
   "metadata": {},
   "source": [
    "## üìä 4. DROID Results Validation\n",
    "\n",
    "Laden und Validierung der generierten DROID CSV-Datei, √úberpr√ºfung auf erwartete Spalten (MD5_HASH, FILE_PATH, FORMAT_NAME) und Analyse der Dateityp-Verteilung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b222dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# DROID RESULTS VALIDATION\n",
    "# =====================================================\n",
    "\n",
    "# DCA File Type Definitions for validation\n",
    "IMG_EXTENSIONS = {\n",
    "    \"jpg\", \"jpeg\", \"tif\", \"tiff\", \"png\", \"gif\", \"bmp\", \"webp\",\n",
    "    \"dng\", \"cr2\", \"cr3\", \"nef\", \"arw\", \"orf\", \"rw2\"  # RAW formats\n",
    "}\n",
    "\n",
    "ADOBE_EXTENSIONS = {\n",
    "    \"psd\", \"psb\", \"ai\", \"indd\", \"idml\", \"eps\", \"pdf\"\n",
    "}\n",
    "\n",
    "CAD_EXTENSIONS = {\n",
    "    \"dwg\", \"dxf\", \"step\", \"stp\", \"iges\", \"igs\", \n",
    "    \"ifc\", \"3dm\", \"skp\"\n",
    "}\n",
    "\n",
    "TARGET_EXTENSIONS = IMG_EXTENSIONS | ADOBE_EXTENSIONS | CAD_EXTENSIONS\n",
    "\n",
    "def load_and_validate_droid_csv():\n",
    "    \"\"\"Load DROID CSV with comprehensive validation\"\"\"\n",
    "    \n",
    "    # Skip if DROID failed\n",
    "    if not pipeline_status.get('droid_success', False):\n",
    "        print(\"‚è≠Ô∏è  Skipping CSV validation - DROID analysis failed\")\n",
    "        return None, {}\n",
    "    \n",
    "    csv_path = pipeline_status['droid_csv_path']\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        error_msg = f\"DROID CSV not found: {csv_path}\"\n",
    "        log_step(\"CSV Validation\", False, error_msg)\n",
    "        return None, {}\n",
    "    \n",
    "    try:\n",
    "        # Load CSV with encoding detection\n",
    "        try:\n",
    "            droid_df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            print(\"‚ö†Ô∏è  UTF-8 failed, trying latin-1 encoding...\")\n",
    "            droid_df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "        \n",
    "        print(f\"üìä DROID CSV loaded: {len(droid_df):,} records\")\n",
    "        print(f\"üìã Columns found: {list(droid_df.columns)}\")\n",
    "        \n",
    "        # Validate expected columns\n",
    "        expected_columns = ['MD5_HASH', 'FILE_PATH', 'FORMAT_NAME', 'NAME', 'EXT', 'SIZE']\n",
    "        missing_columns = []\n",
    "        \n",
    "        for col in expected_columns:\n",
    "            # Check for column variations\n",
    "            found = False\n",
    "            for df_col in droid_df.columns:\n",
    "                if col in df_col.upper():\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                missing_columns.append(col)\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"‚ö†Ô∏è  Missing expected columns: {missing_columns}\")\n",
    "        else:\n",
    "            print(\"‚úÖ All expected columns found\")\n",
    "        \n",
    "        # Analyze data quality\n",
    "        analysis = analyze_droid_data(droid_df)\n",
    "        \n",
    "        # Store in pipeline status\n",
    "        pipeline_status['droid_df'] = droid_df\n",
    "        pipeline_status['file_counts']['droid_records'] = len(droid_df)\n",
    "        \n",
    "        log_step(\"CSV Validation\", True, f\"{len(droid_df):,} records loaded\")\n",
    "        return droid_df, analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to load DROID CSV: {e}\"\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        log_step(\"CSV Validation\", False, error_msg)\n",
    "        return None, {}\n",
    "\n",
    "def analyze_droid_data(df: pd.DataFrame) -> Dict[str, any]:\n",
    "    \"\"\"Comprehensive analysis of DROID data\"\"\"\n",
    "    if df.empty:\n",
    "        return {}\n",
    "    \n",
    "    analysis = {}\n",
    "    \n",
    "    # File type analysis\n",
    "    if 'EXT' in df.columns:\n",
    "        ext_counts = df['EXT'].value_counts()\n",
    "        analysis['extensions'] = ext_counts.head(20).to_dict()\n",
    "        \n",
    "        # Target extensions analysis (for XMP processing)\n",
    "        df['ext_lower'] = df['EXT'].str.lower() if 'EXT' in df.columns else ''\n",
    "        target_mask = df['ext_lower'].isin(TARGET_EXTENSIONS)\n",
    "        analysis['target_files'] = target_mask.sum()\n",
    "        analysis['total_files'] = len(df)\n",
    "    \n",
    "    # MD5 hash analysis\n",
    "    md5_col = None\n",
    "    for col in ['MD5_HASH', 'HASH', 'md5', 'MD5']:\n",
    "        if col in df.columns:\n",
    "            md5_col = col\n",
    "            break\n",
    "    \n",
    "    if md5_col:\n",
    "        analysis['md5_column'] = md5_col\n",
    "        analysis['md5_available'] = df[md5_col].notna().sum()\n",
    "        analysis['md5_missing'] = df[md5_col].isna().sum()\n",
    "    else:\n",
    "        analysis['md5_column'] = None\n",
    "        analysis['md5_available'] = 0\n",
    "        analysis['md5_missing'] = len(df)\n",
    "    \n",
    "    # Size analysis\n",
    "    if 'SIZE' in df.columns:\n",
    "        total_size = df['SIZE'].fillna(0).sum()\n",
    "        analysis['total_size_gb'] = total_size / (1024**3)\n",
    "        analysis['avg_size_mb'] = (df['SIZE'].fillna(0).mean()) / (1024**2)\n",
    "    \n",
    "    # Format analysis\n",
    "    if 'FORMAT_NAME' in df.columns:\n",
    "        format_counts = df['FORMAT_NAME'].value_counts()\n",
    "        analysis['top_formats'] = format_counts.head(10).to_dict()\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Load and validate DROID CSV\n",
    "print(\"üîÑ Loading and validating DROID CSV...\")\n",
    "droid_df, analysis = load_and_validate_droid_csv()\n",
    "\n",
    "if droid_df is not None:\n",
    "    print(f\"\\nüìà DROID Data Analysis:\")\n",
    "    print(f\"üìä Total files: {analysis.get('total_files', 0):,}\")\n",
    "    print(f\"üéØ Target files (img/adobe/cad): {analysis.get('target_files', 0):,}\")\n",
    "    print(f\"#Ô∏è‚É£  MD5 hashes available: {analysis.get('md5_available', 0):,}\")\n",
    "    print(f\"‚ùì MD5 hashes missing: {analysis.get('md5_missing', 0):,}\")\n",
    "    \n",
    "    if analysis.get('md5_column'):\n",
    "        print(f\"üîë MD5 column: '{analysis['md5_column']}'\")\n",
    "    \n",
    "    if analysis.get('total_size_gb'):\n",
    "        print(f\"üíæ Total size: {analysis['total_size_gb']:.2f} GB\")\n",
    "        print(f\"üìè Average file size: {analysis['avg_size_mb']:.2f} MB\")\n",
    "    \n",
    "    if analysis.get('extensions'):\n",
    "        print(f\"\\nüìÅ Top file extensions:\")\n",
    "        for ext, count in list(analysis['extensions'].items())[:10]:\n",
    "            print(f\"   .{ext}: {count:,} files\")\n",
    "    \n",
    "    if analysis.get('top_formats'):\n",
    "        print(f\"\\nüè∑Ô∏è  Top formats:\")\n",
    "        for fmt, count in list(analysis['top_formats'].items())[:5]:\n",
    "            print(f\"   {fmt}: {count:,} files\")\n",
    "    \n",
    "    pipeline_status['analysis'] = analysis\n",
    "    print(f\"\\n‚úÖ DROID CSV validation completed successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ùå DROID CSV validation failed - cannot proceed to RDF conversion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32de5f1",
   "metadata": {},
   "source": [
    "## üèóÔ∏è 5. RDF Graph Initialization\n",
    "\n",
    "Initialisierung des RDF-Graphs mit DCA Ontologie-Namespaces, Hinzuf√ºgung der Kern-Ontologie-Definitionen und Erstellung von Projekt- und Aktivit√§ts-Ressourcen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# RDF GRAPH INITIALIZATION WITH DCA ONTOLOGY\n",
    "# =====================================================\n",
    "\n",
    "# DCA Ontology Namespaces - ETH Z√ºrich Standard\n",
    "DCA      = Namespace(\"http://dca.ethz.ch/ontology#\")     # DCA Classes & Properties\n",
    "DCA_ID   = Namespace(\"http://dca.ethz.ch/id/\")          # Individual Resources\n",
    "DCA_TECH = Namespace(\"http://dca.ethz.ch/tech#\")        # Technical Properties\n",
    "\n",
    "# International Standards\n",
    "PREMIS   = Namespace(\"http://www.loc.gov/premis/rdf/v3/\")  # Digital Preservation Metadata\n",
    "RICO     = Namespace(\"https://www.ica.org/standards/RiC/ontology#\")  # Records in Contexts\n",
    "# DCTERMS already imported from rdflib.namespace\n",
    "\n",
    "# Utility\n",
    "OWL      = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "\n",
    "# Base URI for all DCA identifiers\n",
    "DCA_ID_BASE = \"http://dca.ethz.ch/id/\"\n",
    "\n",
    "def dca_file_uri_from_md5(md5_hex: Optional[str]) -> Optional[URIRef]:\n",
    "    \"\"\"\n",
    "    Creates dca-id:file_<md5[:16]> from MD5 hex string (DROID format).\n",
    "    \n",
    "    Args:\n",
    "        md5_hex: MD5 hash as hex string (32 chars)\n",
    "        \n",
    "    Returns:\n",
    "        URIRef or None if invalid input\n",
    "    \"\"\"\n",
    "    if not md5_hex or not isinstance(md5_hex, str):\n",
    "        return None\n",
    "    \n",
    "    # Clean and validate\n",
    "    md5_clean = md5_hex.strip().lower()\n",
    "    if len(md5_clean) < 16 or not re.match(r'^[a-f0-9]+$', md5_clean):\n",
    "        return None\n",
    "    \n",
    "    # Use first 16 characters for consistent short IDs\n",
    "    short_id = md5_clean[:16]\n",
    "    return URIRef(DCA_ID_BASE + f\"file_{short_id}\")\n",
    "\n",
    "def dca_file_uri_from_path_fallback(file_path: str) -> URIRef:\n",
    "    \"\"\"\n",
    "    Fallback ID generation from file path (when MD5 not available).\n",
    "    \n",
    "    WARNING: Only use this if DROID MD5 is unavailable!\n",
    "    Path-based IDs are less stable than content-based MD5.\n",
    "    \"\"\"\n",
    "    path_normalized = str(Path(file_path)).replace('\\\\\\\\', '/')  # Normalize separators\n",
    "    path_hash = hashlib.sha256(path_normalized.encode('utf-8')).hexdigest()[:16]\n",
    "    return URIRef(DCA_ID_BASE + f\"file_{path_hash}\")\n",
    "\n",
    "def dca_project_uri(project_name: str) -> URIRef:\n",
    "    \"\"\"\n",
    "    Generate project URI following DCA conventions.\n",
    "    \n",
    "    Example: \"WeingutGantenbein\" ‚Üí dca-id:project_WeingutGantenbein\n",
    "    \"\"\"\n",
    "    # Sanitize project name for URI use\n",
    "    sanitized = re.sub(r'[^a-zA-Z0-9_-]', '_', project_name)\n",
    "    return URIRef(DCA_ID_BASE + f\"project_{sanitized}\")\n",
    "\n",
    "def dca_activity_uri(activity_name: str) -> URIRef:\n",
    "    \"\"\"\n",
    "    Generate activity URI for provenance tracking.\n",
    "    \n",
    "    Example: \"ArchivingGantenbein2026\" ‚Üí dca-id:ArchivingGantenbein2026\n",
    "    \"\"\"\n",
    "    sanitized = re.sub(r'[^a-zA-Z0-9_-]', '_', activity_name)\n",
    "    return URIRef(DCA_ID_BASE + sanitized)\n",
    "\n",
    "def create_dca_graph() -> Graph:\n",
    "    \"\"\"\n",
    "    Create RDF graph with DCA ontology structure and namespace bindings.\n",
    "    \"\"\"\n",
    "    g = Graph()\n",
    "    \n",
    "    # Bind namespaces for clean Turtle output\n",
    "    g.bind(\"dca\", DCA)\n",
    "    g.bind(\"dca-id\", DCA_ID)\n",
    "    g.bind(\"dca-tech\", DCA_TECH)\n",
    "    g.bind(\"premis\", PREMIS)\n",
    "    g.bind(\"rico\", RICO)\n",
    "    g.bind(\"dcterms\", DCTERMS)\n",
    "    g.bind(\"owl\", OWL)\n",
    "    g.bind(\"xsd\", XSD)\n",
    "    \n",
    "    return g\n",
    "\n",
    "def add_ontology_definitions(g: Graph):\n",
    "    \"\"\"\n",
    "    Add DCA ontology class and property definitions.\n",
    "    \"\"\"\n",
    "    # Ontology declaration\n",
    "    ontology_uri = URIRef(\"http://dca.ethz.ch/ontology\")\n",
    "    g.add((ontology_uri, RDF.type, OWL.Ontology))\n",
    "    g.add((ontology_uri, DCTERMS.created, Literal(datetime.now().strftime(\"%Y-%m-%d\"), datatype=XSD.date)))\n",
    "    g.add((ontology_uri, DCTERMS.creator, Literal(\"ETH Zurich - Digital Construction Archive Project\")))\n",
    "    g.add((ontology_uri, DCTERMS.description, \n",
    "           Literal(\"Standards-based ontology for digital construction archives using RiC-O, PREMIS, and Dublin Core\", lang=\"en\")))\n",
    "    \n",
    "    # DCA Classes\n",
    "    g.add((DCA.ConstructionProject, RDF.type, OWL.Class))\n",
    "    g.add((DCA.ConstructionProject, RDFS.label, Literal(\"Construction Project\", lang=\"en\")))\n",
    "    g.add((DCA.ConstructionProject, RDFS.comment, \n",
    "           Literal(\"A construction or architectural project containing digital files\", lang=\"en\")))\n",
    "    g.add((DCA.ConstructionProject, RDFS.subClassOf, RICO.RecordSet))\n",
    "    \n",
    "    g.add((DCA.ArchiveFile, RDF.type, OWL.Class))\n",
    "    g.add((DCA.ArchiveFile, RDFS.label, Literal(\"Archive File\", lang=\"en\")))\n",
    "    g.add((DCA.ArchiveFile, RDFS.comment, \n",
    "           Literal(\"A digital file within the construction archive\", lang=\"en\")))\n",
    "    g.add((DCA.ArchiveFile, RDFS.subClassOf, PREMIS.Object))\n",
    "    g.add((DCA.ArchiveFile, RDFS.subClassOf, RICO.Record))\n",
    "    \n",
    "    # DCA Properties\n",
    "    g.add((DCA.belongsToProject, RDF.type, OWL.ObjectProperty))\n",
    "    g.add((DCA.belongsToProject, RDFS.label, Literal(\"belongs to project\", lang=\"en\")))\n",
    "    g.add((DCA.belongsToProject, RDFS.comment, \n",
    "           Literal(\"Indicates that a file belongs to a specific construction project\", lang=\"en\")))\n",
    "    g.add((DCA.belongsToProject, RDFS.domain, DCA.ArchiveFile))\n",
    "    g.add((DCA.belongsToProject, RDFS.range, DCA.ConstructionProject))\n",
    "\n",
    "def add_project_and_activity(g: Graph, project_name: str, activity_name: str):\n",
    "    \"\"\"\n",
    "    Add project and provenance activity to graph.\n",
    "    \"\"\"\n",
    "    # Project\n",
    "    project_uri = dca_project_uri(project_name)\n",
    "    g.add((project_uri, RDF.type, DCA.ConstructionProject))\n",
    "    g.add((project_uri, RDF.type, RICO.RecordSet))\n",
    "    g.add((project_uri, DCTERMS.title, Literal(project_name)))\n",
    "    \n",
    "    # Activity\n",
    "    activity_uri = dca_activity_uri(activity_name)\n",
    "    g.add((activity_uri, RDF.type, RICO.Activity))\n",
    "    g.add((activity_uri, RDFS.label, Literal(f\"Integrierte Archivierung {project_name} Konstruktionsunterlagen\")))\n",
    "    g.add((activity_uri, DCTERMS.description, \n",
    "           Literal(f\"Systematische digitale Erfassung und Archivierung der Konstruktionsdokumentation des {project_name} Projekts mit DROID+XMP Integration.\")))\n",
    "    g.add((activity_uri, RICO.hasEndDate, Literal(\"M√§rz 2026\")))\n",
    "    g.add((activity_uri, RICO.occurredAtDate, Literal(datetime.now().isoformat(), datatype=XSD.dateTime)))\n",
    "    g.add((activity_uri, RICO.technique, \n",
    "           Literal(\"DROID file identification with PRONOM registry, RDF metadata generation, ExifTool XMP integration, integrierte Pipeline\")))\n",
    "    \n",
    "    # Team\n",
    "    team_uri = URIRef(DCA_ID_BASE + \"DCA_Team\")\n",
    "    g.add((team_uri, RDF.type, RICO.Group))\n",
    "    g.add((team_uri, RDFS.label, Literal(\"Digital Construction Archive Team, ETH Z√ºrich\")))\n",
    "    g.add((activity_uri, RICO.isOrWasPerformedBy, team_uri))\n",
    "    \n",
    "    # Link project to activity\n",
    "    g.add((project_uri, RICO.isOrWasDocumentedBy, activity_uri))\n",
    "    \n",
    "    return project_uri, activity_uri\n",
    "\n",
    "# Initialize the RDF graph\n",
    "print(\"üîÑ Initializing DCA RDF Graph...\")\n",
    "graph = create_dca_graph()\n",
    "\n",
    "# Add ontology structure\n",
    "add_ontology_definitions(graph)\n",
    "print(\"‚úÖ DCA ontology definitions added\")\n",
    "\n",
    "# Add project and activity\n",
    "project_uri, activity_uri = add_project_and_activity(graph, PROJECT_NAME, ACTIVITY_NAME)\n",
    "print(f\"‚úÖ Project added: {project_uri}\")\n",
    "print(f\"‚úÖ Activity added: {activity_uri}\")\n",
    "\n",
    "print(f\"üìä Current graph size: {len(graph)} triples\")\n",
    "\n",
    "# Store in pipeline status\n",
    "pipeline_status['graph'] = graph\n",
    "pipeline_status['project_uri'] = project_uri\n",
    "pipeline_status['activity_uri'] = activity_uri\n",
    "\n",
    "log_step(\"RDF Graph Initialization\", True, f\"Graph initialized with {len(graph)} triples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac0420",
   "metadata": {},
   "source": [
    "## üîó 6. DROID CSV to RDF Conversion\n",
    "\n",
    "Verarbeitung jedes DROID-Records zur Erstellung konsistenter MD5-basierter File-URIs, Hinzuf√ºgung von PREMIS-Metadaten, Formatinformationen und NextCloud WebDAV-Identifiern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# DROID CSV TO RDF CONVERSION\n",
    "# =====================================================\n",
    "\n",
    "def safe_literal(value, datatype=None, lang=None):\n",
    "    \"\"\"\n",
    "    Create RDF literal with error handling.\n",
    "    \"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return None\n",
    "    try:\n",
    "        return Literal(str(value), datatype=datatype, lang=lang)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Literal creation failed for '{value}': {e}\")\n",
    "        return Literal(str(value))  # Fallback without datatype\n",
    "\n",
    "def safe_add_triple(g: Graph, subject, predicate, obj):\n",
    "    \"\"\"\n",
    "    Safely add triple to graph, only if all components are not None.\n",
    "    \"\"\"\n",
    "    if subject is not None and predicate is not None and obj is not None:\n",
    "        g.add((subject, predicate, obj))\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def add_premis_identifier(g: Graph, file_uri: URIRef, id_type: str, value: str):\n",
    "    \"\"\"\n",
    "    Add PREMIS identifier as blank node.\n",
    "    \"\"\"\n",
    "    if not value or pd.isna(value):\n",
    "        return\n",
    "    \n",
    "    bn = BNode()\n",
    "    g.add((file_uri, PREMIS.hasIdentifier, bn))\n",
    "    g.add((bn, PREMIS.identifierType, Literal(id_type)))\n",
    "    g.add((bn, PREMIS.identifierValue, Literal(str(value))))\n",
    "\n",
    "def process_droid_record(g: Graph, record: pd.Series, project_uri: URIRef) -> Optional[URIRef]:\n",
    "    \"\"\"\n",
    "    Process single DROID record and add to RDF graph.\n",
    "    \n",
    "    Returns the created file URI or None if processing failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine MD5 hash column (DROID CSV variations)\n",
    "        md5_hash = None\n",
    "        for col in ['MD5_HASH', 'HASH', 'md5', 'MD5']:\n",
    "            if col in record.index and not pd.isna(record.get(col)):\n",
    "                md5_hash = record[col]\n",
    "                break\n",
    "        \n",
    "        # Generate file URI\n",
    "        if md5_hash:\n",
    "            file_uri = dca_file_uri_from_md5(md5_hash)\n",
    "        else:\n",
    "            # Fallback to path-based ID\n",
    "            file_path = record.get('FILE_PATH', record.get('PATH', ''))\n",
    "            if not file_path:\n",
    "                return None\n",
    "            file_uri = dca_file_uri_from_path_fallback(file_path)\n",
    "        \n",
    "        if not file_uri:\n",
    "            return None\n",
    "        \n",
    "        # Core classes\n",
    "        g.add((file_uri, RDF.type, DCA.ArchiveFile))\n",
    "        g.add((file_uri, RDF.type, PREMIS.Object))\n",
    "        g.add((file_uri, RDF.type, RICO.Record))\n",
    "        \n",
    "        # Project relationship\n",
    "        g.add((file_uri, DCA.belongsToProject, project_uri))\n",
    "        g.add((file_uri, RICO.isOrWasIncludedIn, project_uri))\n",
    "        \n",
    "        # Basic metadata\n",
    "        if 'NAME' in record.index:\n",
    "            title_literal = safe_literal(record['NAME'])\n",
    "            if title_literal:\n",
    "                g.add((file_uri, DCTERMS.title, title_literal))\n",
    "        \n",
    "        # File path as identifier (WebDAV-style)\n",
    "        file_path = record.get('FILE_PATH', record.get('PATH', ''))\n",
    "        if file_path:\n",
    "            # Convert to WebDAV URL format\n",
    "            webdav_url = f\"https://nextcloud.ethz.ch/remote.php/dav/files/padrian/DCA/{PROJECT_NAME}/{file_path}\"\n",
    "            g.add((file_uri, DCTERMS.identifier, URIRef(webdav_url)))\n",
    "        \n",
    "        # Timestamps\n",
    "        for time_col in ['LAST_MODIFIED', 'MODIFIED', 'DATE_MODIFIED']:\n",
    "            if time_col in record.index and not pd.isna(record[time_col]):\n",
    "                timestamp = safe_literal(record[time_col], datatype=XSD.dateTime)\n",
    "                if timestamp:\n",
    "                    g.add((file_uri, DCTERMS.modified, timestamp))\n",
    "                break\n",
    "        \n",
    "        # PREMIS format information\n",
    "        if 'FORMAT_NAME' in record.index:\n",
    "            format_literal = safe_literal(record['FORMAT_NAME'])\n",
    "            if format_literal:\n",
    "                g.add((file_uri, PREMIS.hasFormatName, format_literal))\n",
    "        \n",
    "        # Format details\n",
    "        format_notes = []\n",
    "        if 'MIME_TYPE' in record.index and not pd.isna(record['MIME_TYPE']):\n",
    "            format_notes.append(f\"MIME: {record['MIME_TYPE']}\")\n",
    "        if 'PUID' in record.index and not pd.isna(record['PUID']):\n",
    "            format_notes.append(f\"PRONOM ID: {record['PUID']}\")\n",
    "        \n",
    "        for note in format_notes:\n",
    "            g.add((file_uri, PREMIS.hasFormatNote, Literal(note)))\n",
    "        \n",
    "        # File size\n",
    "        if 'SIZE' in record.index and not pd.isna(record['SIZE']):\n",
    "            try:\n",
    "                size_val = int(float(record['SIZE']))\n",
    "                g.add((file_uri, PREMIS.hasSize, Literal(size_val, datatype=XSD.long)))\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "        \n",
    "        # DROID identification method\n",
    "        g.add((file_uri, PREMIS.hasCreatingApplication, Literal(\"DROID: Signature\")))\n",
    "        \n",
    "        # Store MD5 for later XMP matching\n",
    "        if md5_hash:\n",
    "            add_premis_identifier(g, file_uri, \"MD5 Hash\", md5_hash)\n",
    "        \n",
    "        return file_uri\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to process record: {e}\")\n",
    "        return None\n",
    "\n",
    "def convert_droid_to_rdf():\n",
    "    \"\"\"Convert DROID CSV to RDF with comprehensive validation\"\"\"\n",
    "    \n",
    "    # Skip if prerequisites not met\n",
    "    if 'graph' not in pipeline_status or 'droid_df' not in pipeline_status:\n",
    "        print(\"‚è≠Ô∏è  Skipping RDF conversion - prerequisites not met\")\n",
    "        return False\n",
    "    \n",
    "    g = pipeline_status['graph']\n",
    "    droid_df = pipeline_status['droid_df']\n",
    "    project_uri = pipeline_status['project_uri']\n",
    "    \n",
    "    if droid_df.empty:\n",
    "        print(\"‚ùå No DROID data to process\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"üîÑ Converting {len(droid_df):,} DROID records to RDF...\")\n",
    "    \n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    file_uris = []  # Store for later XMP processing\n",
    "    \n",
    "    # Progress tracking\n",
    "    total_records = len(droid_df)\n",
    "    batch_size = 1000\n",
    "    \n",
    "    for idx, record in droid_df.iterrows():\n",
    "        file_uri = process_droid_record(g, record, project_uri)\n",
    "        if file_uri:\n",
    "            processed_count += 1\n",
    "            file_uris.append(str(file_uri))\n",
    "        else:\n",
    "            error_count += 1\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (idx + 1) % batch_size == 0:\n",
    "            progress = ((idx + 1) / total_records) * 100\n",
    "            print(f\"   Progress: {idx + 1:,} / {total_records:,} records ({progress:.1f}%)\")\n",
    "    \n",
    "    # Final validation\n",
    "    current_triples = len(g)\n",
    "    added_triples = current_triples - pipeline_status.get('initial_triples', 0)\n",
    "    \n",
    "    print(f\"‚úÖ DROID to RDF conversion completed:\")\n",
    "    print(f\"   üìä Successfully processed: {processed_count:,} files\")\n",
    "    print(f\"   ‚ùå Errors: {error_count:,} files\")\n",
    "    print(f\"   üìà Graph size: {current_triples:,} triples (+{added_triples:,})\")\n",
    "    \n",
    "    # Store results\n",
    "    pipeline_status['file_counts']['rdf_files'] = processed_count\n",
    "    pipeline_status['file_counts']['conversion_errors'] = error_count\n",
    "    pipeline_status['file_uris'] = file_uris\n",
    "    pipeline_status['graph'] = g\n",
    "    \n",
    "    success = error_count < (processed_count * 0.1)  # Less than 10% errors\n",
    "    log_step(\"DROID to RDF Conversion\", success, f\"{processed_count:,} files converted\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "# Execute DROID to RDF conversion\n",
    "if pipeline_status.get('droid_df') is not None:\n",
    "    pipeline_status['initial_triples'] = len(pipeline_status['graph'])\n",
    "    conversion_success = convert_droid_to_rdf()\n",
    "    pipeline_status['rdf_conversion_success'] = conversion_success\n",
    "    \n",
    "    if conversion_success:\n",
    "        print(f\"\\nüéØ DROID to RDF conversion completed successfully!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  DROID to RDF conversion completed with errors\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping DROID to RDF conversion - no DROID data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e9cbd",
   "metadata": {},
   "source": [
    "## üîç 7. RDF Content Validation\n",
    "\n",
    "Validierung der generierten RDF mittels SPARQL-Queries, √úberpr√ºfung von Triple-Anzahlen, Verifizierung der File-URI-Konsistenz und Sicherstellung, dass alle DROID-Records verarbeitet wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951815de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# RDF CONTENT VALIDATION\n",
    "# =====================================================\n",
    "\n",
    "def validate_rdf_content():\n",
    "    \"\"\"Comprehensive RDF validation using SPARQL queries\"\"\"\n",
    "    \n",
    "    if 'graph' not in pipeline_status:\n",
    "        print(\"‚è≠Ô∏è  Skipping RDF validation - no graph available\")\n",
    "        return False\n",
    "    \n",
    "    g = pipeline_status['graph']\n",
    "    \n",
    "    print(\"üîç Validating RDF content with SPARQL queries...\")\n",
    "    \n",
    "    validation_queries = {\n",
    "        \"Total Files\": \"\"\"\n",
    "            PREFIX dca: <http://dca.ethz.ch/ontology#>\n",
    "            SELECT (COUNT(?file) AS ?count) WHERE {\n",
    "                ?file a dca:ArchiveFile .\n",
    "            }\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Files with Titles\": \"\"\"\n",
    "            PREFIX dca: <http://dca.ethz.ch/ontology#>\n",
    "            PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "            SELECT (COUNT(?file) AS ?count) WHERE {\n",
    "                ?file a dca:ArchiveFile ;\n",
    "                      dcterms:title ?title .\n",
    "            }\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Files with Identifiers\": \"\"\"\n",
    "            PREFIX dca: <http://dca.ethz.ch/ontology#>  \n",
    "            PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "            SELECT (COUNT(?file) AS ?count) WHERE {\n",
    "                ?file a dca:ArchiveFile ;\n",
    "                      dcterms:identifier ?identifier .\n",
    "            }\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Files with Format Info\": \"\"\"\n",
    "            PREFIX dca: <http://dca.ethz.ch/ontology#>\n",
    "            PREFIX premis: <http://www.loc.gov/premis/rdf/v3/>\n",
    "            SELECT (COUNT(?file) AS ?count) WHERE {\n",
    "                ?file a dca:ArchiveFile ;\n",
    "                      premis:hasFormatName ?format .\n",
    "            }\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Files with MD5 Hashes\": \"\"\"\n",
    "            PREFIX dca: <http://dca.ethz.ch/ontology#>\n",
    "            PREFIX premis: <http://www.loc.gov/premis/rdf/v3/>\n",
    "            SELECT (COUNT(?file) AS ?count) WHERE {\n",
    "                ?file a dca:ArchiveFile .\n",
    "                ?file premis:hasIdentifier ?id .\n",
    "                ?id premis:identifierType \"MD5 Hash\" .\n",
    "            }\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Project Info\": \"\"\"\n",
    "            PREFIX dca: <http://dca.ethz.ch/ontology#>\n",
    "            PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "            SELECT ?project ?title (COUNT(?file) AS ?fileCount) WHERE {\n",
    "                ?project a dca:ConstructionProject ;\n",
    "                         dcterms:title ?title .\n",
    "                ?file dca:belongsToProject ?project .\n",
    "            } GROUP BY ?project ?title\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Top Formats\": \"\"\"\n",
    "            PREFIX dca: <http://dca.ethz.ch/ontology#>\n",
    "            PREFIX premis: <http://www.loc.gov/premis/rdf/v3/>\n",
    "            SELECT ?format (COUNT(?file) AS ?count) WHERE {\n",
    "                ?file a dca:ArchiveFile ;\n",
    "                      premis:hasFormatName ?format .\n",
    "            } GROUP BY ?format ORDER BY DESC(?count) LIMIT 10\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    print(\"\\\\nüìä RDF Validation Results:\")\n",
    "    \n",
    "    for query_name, query_text in validation_queries.items():\n",
    "        try:\n",
    "            results = list(g.query(query_text))\n",
    "            \n",
    "            if query_name in [\"Total Files\", \"Files with Titles\", \"Files with Identifiers\", \n",
    "                            \"Files with Format Info\", \"Files with MD5 Hashes\"]:\n",
    "                if results:\n",
    "                    count = int(results[0][0])\n",
    "                    validation_results[query_name] = count\n",
    "                    print(f\"   ‚úÖ {query_name}: {count:,}\")\n",
    "                else:\n",
    "                    validation_results[query_name] = 0\n",
    "                    print(f\"   ‚ùå {query_name}: 0\")\n",
    "            \n",
    "            elif query_name == \"Project Info\":\n",
    "                for row in results:\n",
    "                    print(f\"   üìÇ Project: {row.title} ({int(row.fileCount):,} files)\")\n",
    "                    validation_results[query_name] = int(row.fileCount)\n",
    "            \n",
    "            elif query_name == \"Top Formats\":\n",
    "                print(f\"   üè∑Ô∏è  Top File Formats:\")\n",
    "                for i, row in enumerate(results[:5]):\n",
    "                    print(f\"      {i+1}. {row.format}: {int(row.count):,} files\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {query_name}: Query failed - {e}\")\n",
    "            validation_results[query_name] = f\"Error: {e}\"\n",
    "    \n",
    "    # Calculate coverage percentages\n",
    "    total_files = validation_results.get(\"Total Files\", 0)\n",
    "    if total_files > 0:\n",
    "        print(f\"\\\\nüìà Metadata Coverage Analysis:\")\n",
    "        \n",
    "        coverage_metrics = {\n",
    "            \"Titles\": validation_results.get(\"Files with Titles\", 0),\n",
    "            \"Identifiers\": validation_results.get(\"Files with Identifiers\", 0), \n",
    "            \"Format Info\": validation_results.get(\"Files with Format Info\", 0),\n",
    "            \"MD5 Hashes\": validation_results.get(\"Files with MD5 Hashes\", 0)\n",
    "        }\n",
    "        \n",
    "        for metric, count in coverage_metrics.items():\n",
    "            if isinstance(count, int):\n",
    "                coverage = (count / total_files) * 100\n",
    "                print(f\"   {metric}: {coverage:.1f}% ({count:,}/{total_files:,})\")\n",
    "    \n",
    "    # Validation checks\n",
    "    expected_files = pipeline_status.get('file_counts', {}).get('rdf_files', 0)\n",
    "    actual_files = validation_results.get(\"Total Files\", 0)\n",
    "    \n",
    "    validation_passed = True\n",
    "    validation_issues = []\n",
    "    \n",
    "    # Check file count consistency\n",
    "    if actual_files != expected_files:\n",
    "        validation_issues.append(f\"File count mismatch: expected {expected_files}, got {actual_files}\")\n",
    "        validation_passed = False\n",
    "    \n",
    "    # Check minimum coverage requirements\n",
    "    min_coverage = 0.95  # 95% minimum coverage\n",
    "    for metric, count in [(\"Titles\", validation_results.get(\"Files with Titles\", 0)),\n",
    "                          (\"Identifiers\", validation_results.get(\"Files with Identifiers\", 0))]:\n",
    "        if isinstance(count, int) and total_files > 0:\n",
    "            coverage = count / total_files\n",
    "            if coverage < min_coverage:\n",
    "                validation_issues.append(f\"{metric} coverage too low: {coverage:.1%} < {min_coverage:.1%}\")\n",
    "                validation_passed = False\n",
    "    \n",
    "    # Show validation summary\n",
    "    if validation_passed:\n",
    "        print(f\"\\\\n‚úÖ RDF Content Validation PASSED\")\n",
    "        log_step(\"RDF Content Validation\", True, f\"{total_files:,} files validated\")\n",
    "    else:\n",
    "        print(f\"\\\\n‚ö†Ô∏è  RDF Content Validation FAILED:\")\n",
    "        for issue in validation_issues:\n",
    "            print(f\"   ‚Ä¢ {issue}\")\n",
    "        log_step(\"RDF Content Validation\", False, f\"{len(validation_issues)} issues found\")\n",
    "    \n",
    "    pipeline_status['validation_results'] = validation_results\n",
    "    pipeline_status['validation_passed'] = validation_passed\n",
    "    \n",
    "    return validation_passed\n",
    "\n",
    "# Execute RDF validation\n",
    "if pipeline_status.get('rdf_conversion_success'):\n",
    "    validation_success = validate_rdf_content()\n",
    "    \n",
    "    if validation_success:\n",
    "        print(f\"\\\\nüéØ RDF content validation completed successfully!\")\n",
    "    else:\n",
    "        print(f\"\\\\n‚ö†Ô∏è  RDF content validation found issues - review before proceeding\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping RDF validation - RDF conversion not successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce487ef",
   "metadata": {},
   "source": [
    "## üì∏ 8. ExifTool XMP Data Extraction\n",
    "\n",
    "Ausf√ºhrung von ExifTool im Batch-Modus zur Extraktion von XMP-Metadaten aus Bild- und Adobe-Dateien, Fokus auf DocumentID, InstanceID und Derivation-Beziehungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa96f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# EXIFTOOL XMP DATA EXTRACTION  \n",
    "# =====================================================\n",
    "\n",
    "def run_exiftool_json(files: List[str], fast=False) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run ExifTool as JSON with error tolerance and UTF-8 filenames\n",
    "    \"\"\"\n",
    "    if not files:\n",
    "        return []\n",
    "    \n",
    "    cmd = [exiftool_command, \"-a\", \"-s\", \"-G1\", \"-json\", \"-charset\", \"filename=UTF8\", \"-m\"]\n",
    "    if fast:\n",
    "        cmd.insert(-1, \"-fast\")\n",
    "    \n",
    "    # Specific XMP tags we need\n",
    "    xmp_tags = [\n",
    "        \"XMP-xmpMM:DocumentID\",\n",
    "        \"XMP-xmpMM:InstanceID\", \n",
    "        \"XMP-xmpMM:OriginalDocumentID\",\n",
    "        \"XMP-xmpMM:DerivedFromDocumentID\",\n",
    "        \"XMP-xmpMM:DerivedFromInstanceID\",\n",
    "        \"XMP-xmp:CreatorTool\",\n",
    "        \"File:FileName\",\n",
    "        \"File:Directory\",\n",
    "        \"File:FileModifyDate\"\n",
    "    ]\n",
    "    \n",
    "    cmd.extend(xmp_tags)\n",
    "    cmd.extend(files)\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, text=True, capture_output=True, timeout=300)\n",
    "        output = result.stdout.strip()\n",
    "        \n",
    "        if output:\n",
    "            return json.loads(output)\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        logger.warning(\"ExifTool timeout - skipping batch\")\n",
    "        return []\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.warning(f\"ExifTool JSON parse error: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"ExifTool execution error: {e}\")\n",
    "        return []\n",
    "\n",
    "def find_target_files_from_rdf():\n",
    "    \"\"\"Find target files (images/Adobe) from RDF data\"\"\"\n",
    "    \n",
    "    if 'graph' not in pipeline_status:\n",
    "        return []\n",
    "    \n",
    "    g = pipeline_status['graph']\n",
    "    \n",
    "    # SPARQL query to find target file types from RDF\n",
    "    target_query = \"\"\"\n",
    "        PREFIX dca: <http://dca.ethz.ch/ontology#>\n",
    "        PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "        PREFIX premis: <http://www.loc.gov/premis/rdf/v3/>\n",
    "        \n",
    "        SELECT ?file ?title ?identifier ?format WHERE {\n",
    "            ?file a dca:ArchiveFile ;\n",
    "                  dcterms:title ?title ;\n",
    "                  dcterms:identifier ?identifier .\n",
    "            OPTIONAL { ?file premis:hasFormatName ?format }\n",
    "            \n",
    "            # Filter for image/Adobe files by extension\n",
    "            FILTER(\n",
    "                CONTAINS(LCASE(?title), \".jpg\") ||\n",
    "                CONTAINS(LCASE(?title), \".jpeg\") ||\n",
    "                CONTAINS(LCASE(?title), \".tif\") ||\n",
    "                CONTAINS(LCASE(?title), \".tiff\") ||\n",
    "                CONTAINS(LCASE(?title), \".png\") ||\n",
    "                CONTAINS(LCASE(?title), \".psd\") ||\n",
    "                CONTAINS(LCASE(?title), \".psb\") ||\n",
    "                CONTAINS(LCASE(?title), \".ai\") ||\n",
    "                CONTAINS(LCASE(?title), \".pdf\") ||\n",
    "                CONTAINS(LCASE(?title), \".indd\")\n",
    "            )\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    target_files = []\n",
    "    \n",
    "    for row in g.query(target_query):\n",
    "        file_uri = str(row.file)\n",
    "        title = str(row.title)\n",
    "        identifier = str(row.identifier)\n",
    "        format_name = str(row.format) if row.format else \"Unknown\"\n",
    "        \n",
    "        # Convert NextCloud URL to local path\n",
    "        local_path = None\n",
    "        if identifier.startswith(\"https://nextcloud.ethz.ch/\"):\n",
    "            try:\n",
    "                # Extract path after dataset name  \n",
    "                if dataset_to_analyze in identifier:\n",
    "                    parts = identifier.split(f\"{dataset_to_analyze}/\")\n",
    "                    if len(parts) > 1:\n",
    "                        path_part = unquote(parts[-1])\n",
    "                        local_path = files_base_dir / path_part\n",
    "                        \n",
    "                        # Check if file exists locally\n",
    "                        if local_path.exists():\n",
    "                            target_files.append({\n",
    "                                'file_uri': file_uri,\n",
    "                                'title': title,\n",
    "                                'local_path': str(local_path),\n",
    "                                'format': format_name,\n",
    "                                'exists': True\n",
    "                            })\n",
    "                        else:\n",
    "                            target_files.append({\n",
    "                                'file_uri': file_uri,\n",
    "                                'title': title, \n",
    "                                'local_path': str(local_path),\n",
    "                                'format': format_name,\n",
    "                                'exists': False\n",
    "                            })\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Path conversion failed for {identifier}: {e}\")\n",
    "    \n",
    "    return target_files\n",
    "\n",
    "def extract_xmp_metadata():\n",
    "    \"\"\"Extract XMP metadata using ExifTool in batch mode\"\"\"\n",
    "    \n",
    "    print(\"üîç Finding target files for XMP extraction...\")\n",
    "    target_files = find_target_files_from_rdf()\n",
    "    \n",
    "    if not target_files:\n",
    "        print(\"‚ùå No target files found for XMP extraction\")\n",
    "        return []\n",
    "    \n",
    "    # Filter to existing files only\n",
    "    existing_files = [f for f in target_files if f['exists']]\n",
    "    \n",
    "    print(f\"üìä Target files analysis:\")\n",
    "    print(f\"   Total target files: {len(target_files):,}\")\n",
    "    print(f\"   Files exist locally: {len(existing_files):,}\")\n",
    "    print(f\"   Missing files: {len(target_files) - len(existing_files):,}\")\n",
    "    \n",
    "    if not existing_files:\n",
    "        print(\"‚ùå No local files available for XMP extraction\")\n",
    "        return []\n",
    "    \n",
    "    # Batch processing setup\n",
    "    BATCH_SIZE = 50  # Smaller batches for ExifTool\n",
    "    all_xmp_records = []\n",
    "    total_files = len(existing_files)\n",
    "    \n",
    "    print(f\"üîÑ Starting XMP extraction for {total_files:,} files in batches of {BATCH_SIZE}...\")\n",
    "    \n",
    "    file_paths = [f['local_path'] for f in existing_files]\n",
    "    \n",
    "    for i in range(0, len(file_paths), BATCH_SIZE):\n",
    "        batch_paths = file_paths[i:i+BATCH_SIZE]\n",
    "        batch_num = (i // BATCH_SIZE) + 1\n",
    "        total_batches = math.ceil(len(file_paths) / BATCH_SIZE)\n",
    "        \n",
    "        print(f\"   üì¶ Processing batch {batch_num}/{total_batches} ({len(batch_paths)} files)...\")\n",
    "        \n",
    "        # Extract XMP metadata for batch\n",
    "        xmp_results = run_exiftool_json(batch_paths)\n",
    "        \n",
    "        for xmp_data in xmp_results:\n",
    "            record = {\n",
    "                \"SourceFile\": xmp_data.get(\"SourceFile\"),\n",
    "                \"DocumentID\": xmp_data.get(\"XMP-xmpMM:DocumentID\"),\n",
    "                \"InstanceID\": xmp_data.get(\"XMP-xmpMM:InstanceID\"),\n",
    "                \"OriginalDocumentID\": xmp_data.get(\"XMP-xmpMM:OriginalDocumentID\"),\n",
    "                \"DerivedFromDocumentID\": xmp_data.get(\"XMP-xmpMM:DerivedFromDocumentID\"),\n",
    "                \"DerivedFromInstanceID\": xmp_data.get(\"XMP-xmpMM:DerivedFromInstanceID\"),\n",
    "                \"CreatorTool\": xmp_data.get(\"XMP-xmp:CreatorTool\"),\n",
    "                \"FileModifyDate\": xmp_data.get(\"File:FileModifyDate\")\n",
    "            }\n",
    "            all_xmp_records.append(record)\n",
    "        \n",
    "        progress = min(i + BATCH_SIZE, total_files)\n",
    "        print(f\"      Progress: {progress:,}/{total_files:,} files processed\")\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    xmp_df = pd.DataFrame(all_xmp_records)\n",
    "    \n",
    "    # Analysis of extracted data\n",
    "    print(f\"\\\\n‚úÖ XMP extraction completed:\")\n",
    "    print(f\"   üìä Files processed: {len(xmp_df):,}\")\n",
    "    \n",
    "    if not xmp_df.empty:\n",
    "        # Count files with XMP metadata\n",
    "        has_doc_id = xmp_df['DocumentID'].notna().sum()\n",
    "        has_inst_id = xmp_df['InstanceID'].notna().sum()\n",
    "        has_derived = (xmp_df['DerivedFromDocumentID'].notna() | \n",
    "                      xmp_df['DerivedFromInstanceID'].notna()).sum()\n",
    "        \n",
    "        print(f\"   üîó Files with DocumentID: {has_doc_id:,}\")\n",
    "        print(f\"   üÜî Files with InstanceID: {has_inst_id:,}\")\n",
    "        print(f\"   üìé Files with derivation info: {has_derived:,}\")\n",
    "        \n",
    "        # Creator tool analysis\n",
    "        if xmp_df['CreatorTool'].notna().sum() > 0:\n",
    "            creator_tools = xmp_df['CreatorTool'].value_counts().head(5)\n",
    "            print(f\"   üîß Top creator tools:\")\n",
    "            for tool, count in creator_tools.items():\n",
    "                print(f\"      {tool}: {count:,} files\")\n",
    "    \n",
    "    # Store results\n",
    "    pipeline_status['xmp_df'] = xmp_df\n",
    "    pipeline_status['target_files'] = existing_files\n",
    "    pipeline_status['file_counts']['xmp_processed'] = len(xmp_df)\n",
    "    pipeline_status['file_counts']['xmp_with_metadata'] = has_doc_id if 'has_doc_id' in locals() else 0\n",
    "    \n",
    "    log_step(\"XMP Extraction\", True, f\"{len(xmp_df):,} files processed\")\n",
    "    \n",
    "    return xmp_df\n",
    "\n",
    "# Execute XMP extraction\n",
    "if pipeline_status.get('validation_passed', False):\n",
    "    xmp_df = extract_xmp_metadata()\n",
    "    \n",
    "    if not xmp_df.empty:\n",
    "        print(f\"\\\\nüéØ XMP extraction completed successfully!\")\n",
    "        print(f\"   Ready for integration into RDF graph\")\n",
    "    else:\n",
    "        print(f\"\\\\n‚ö†Ô∏è  XMP extraction completed but no metadata found\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping XMP extraction - RDF validation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243c84b",
   "metadata": {},
   "source": [
    "## üîó 9. XMP Data Integration into RDF\n",
    "\n",
    "Merge von XMP-Metadaten in den bestehenden RDF-Graph mit denselben MD5-basierten File-URIs, Hinzuf√ºgung von PREMIS-Identifiern und Erstellung von Derivation-Beziehungen zwischen Dateien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a951fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# XMP DATA INTEGRATION INTO RDF\n",
    "# =====================================================\n",
    "\n",
    "def build_path_to_md5_mapping():\n",
    "    \"\"\"Build mapping from file paths to MD5 hashes from DROID data\"\"\"\n",
    "    path_to_md5 = {}\n",
    "    \n",
    "    if 'droid_df' not in pipeline_status:\n",
    "        return path_to_md5\n",
    "    \n",
    "    droid_df = pipeline_status['droid_df']\n",
    "    \n",
    "    # Find MD5 column\n",
    "    md5_col = None\n",
    "    for col in ['MD5_HASH', 'HASH', 'md5', 'MD5']:\n",
    "        if col in droid_df.columns:\n",
    "            md5_col = col\n",
    "            break\n",
    "    \n",
    "    if md5_col:\n",
    "        for _, record in droid_df.iterrows():\n",
    "            file_path = record.get('FILE_PATH', record.get('PATH', ''))\n",
    "            md5_hash = record.get(md5_col)\n",
    "            \n",
    "            if file_path and md5_hash and not pd.isna(md5_hash):\n",
    "                # Create full path\n",
    "                full_path = str(files_base_dir / file_path)\n",
    "                path_to_md5[full_path] = md5_hash\n",
    "    \n",
    "    return path_to_md5\n",
    "\n",
    "def get_file_uri_from_path(file_path: str, path_to_md5: Dict[str, str]) -> URIRef:\n",
    "    \"\"\"Get consistent file URI from path using MD5 or fallback\"\"\"\n",
    "    # Try MD5-based URI first\n",
    "    md5_hash = path_to_md5.get(file_path)\n",
    "    if md5_hash:\n",
    "        return dca_file_uri_from_md5(md5_hash)\n",
    "    \n",
    "    # Fallback to path-based URI\n",
    "    return dca_file_uri_from_path_fallback(file_path)\n",
    "\n",
    "def integrate_xmp_into_rdf():\n",
    "    \"\"\"Integrate XMP metadata into existing RDF graph\"\"\"\n",
    "    \n",
    "    # Prerequisites check\n",
    "    if ('graph' not in pipeline_status or \n",
    "        'xmp_df' not in pipeline_status):\n",
    "        print(\"‚è≠Ô∏è  Skipping XMP integration - prerequisites not met\")\n",
    "        return False\n",
    "    \n",
    "    g = pipeline_status['graph']\n",
    "    xmp_df = pipeline_status['xmp_df']\n",
    "    \n",
    "    if xmp_df.empty:\n",
    "        print(\"‚ö†Ô∏è  No XMP data to integrate\")\n",
    "        return True  # Not an error, just no data\n",
    "    \n",
    "    print(f\"üîÑ Integrating XMP metadata into RDF graph...\")\n",
    "    \n",
    "    # Build path-to-MD5 mapping for consistent URIs\n",
    "    path_to_md5 = build_path_to_md5_mapping()\n",
    "    print(f\"üìã Built MD5 mapping for {len(path_to_md5):,} files\")\n",
    "    \n",
    "    # Build indices for derivation matching\n",
    "    doc_id_to_path = {}\n",
    "    inst_id_to_path = {}\n",
    "    \n",
    "    for _, row in xmp_df.iterrows():\n",
    "        source_file = row['SourceFile']\n",
    "        if not source_file:\n",
    "            continue\n",
    "            \n",
    "        if row['DocumentID']:\n",
    "            doc_id_to_path[row['DocumentID']] = source_file\n",
    "        if row['InstanceID']:\n",
    "            inst_id_to_path[row['InstanceID']] = source_file\n",
    "    \n",
    "    print(f\"üìá Built indexing: {len(doc_id_to_path)} DocumentIDs, {len(inst_id_to_path)} InstanceIDs\")\n",
    "    \n",
    "    # Integration counters\n",
    "    added_identifiers = 0\n",
    "    added_derivations = 0\n",
    "    processed_files = 0\n",
    "    skipped_duplicates = 0\n",
    "    \n",
    "    # Track processed identifiers per file to avoid duplicates\n",
    "    processed_per_file = {}\n",
    "    \n",
    "    initial_triples = len(g)\n",
    "    \n",
    "    for _, row in xmp_df.iterrows():\n",
    "        source_file = row['SourceFile']\n",
    "        if not source_file:\n",
    "            continue\n",
    "            \n",
    "        # Get consistent file URI\n",
    "        file_uri = get_file_uri_from_path(source_file, path_to_md5)\n",
    "        file_uri_str = str(file_uri)\n",
    "        \n",
    "        # Initialize tracking for this file\n",
    "        if file_uri_str not in processed_per_file:\n",
    "            processed_per_file[file_uri_str] = set()\n",
    "        \n",
    "        # Ensure file is properly typed in RDF\n",
    "        g.add((file_uri, RDF.type, DCA.ArchiveFile))\n",
    "        g.add((file_uri, RDF.type, PREMIS.Object))\n",
    "        g.add((file_uri, RDF.type, RICO.Record))\n",
    "        \n",
    "        processed_files += 1\n",
    "        \n",
    "        # Add XMP identifiers (with duplicate prevention)\n",
    "        xmp_identifiers = [\n",
    "            (\"XMP DocumentID\", row['DocumentID']),\n",
    "            (\"XMP InstanceID\", row['InstanceID']),\n",
    "            (\"XMP OriginalDocumentID\", row['OriginalDocumentID'])\n",
    "        ]\n",
    "        \n",
    "        for id_type, id_value in xmp_identifiers:\n",
    "            if id_value and not pd.isna(id_value):\n",
    "                id_key = f\"{id_type}:{id_value}\"\n",
    "                if id_key not in processed_per_file[file_uri_str]:\n",
    "                    add_premis_identifier(g, file_uri, id_type, str(id_value))\n",
    "                    processed_per_file[file_uri_str].add(id_key)\n",
    "                    added_identifiers += 1\n",
    "                else:\n",
    "                    skipped_duplicates += 1\n",
    "        \n",
    "        # Add CreatorTool if available\n",
    "        if row['CreatorTool'] and not pd.isna(row['CreatorTool']):\n",
    "            g.add((file_uri, PREMIS.hasCreatingApplication, Literal(str(row['CreatorTool']))))\n",
    "        \n",
    "        # Add derivation relationships\n",
    "        parent_path = None\n",
    "        \n",
    "        # Try DocumentID first, then InstanceID for parent matching\n",
    "        if row['DerivedFromDocumentID'] and row['DerivedFromDocumentID'] in doc_id_to_path:\n",
    "            parent_path = doc_id_to_path[row['DerivedFromDocumentID']]\n",
    "        elif row['DerivedFromInstanceID'] and row['DerivedFromInstanceID'] in inst_id_to_path:\n",
    "            parent_path = inst_id_to_path[row['DerivedFromInstanceID']]\n",
    "        \n",
    "        if parent_path:\n",
    "            parent_uri = get_file_uri_from_path(parent_path, path_to_md5)\n",
    "            \n",
    "            # Add bidirectional derivation relationships\n",
    "            g.add((file_uri, PREMIS.hasSource, parent_uri))\n",
    "            g.add((parent_uri, PREMIS.isSourceOf, file_uri))\n",
    "            added_derivations += 1\n",
    "    \n",
    "    # Final statistics\n",
    "    final_triples = len(g)\n",
    "    added_triples = final_triples - initial_triples\n",
    "    \n",
    "    print(f\"‚úÖ XMP integration completed:\")\n",
    "    print(f\"   üìÅ Files processed: {processed_files:,}\")\n",
    "    print(f\"   üîó XMP identifiers added: {added_identifiers:,}\")\n",
    "    print(f\"   üìé Derivation relationships: {added_derivations:,}\")\n",
    "    print(f\"   üîÑ Duplicate identifiers skipped: {skipped_duplicates:,}\")\n",
    "    print(f\"   üìà Triples added: {added_triples:,}\")\n",
    "    print(f\"   üìä Total graph size: {final_triples:,} triples\")\n",
    "    \n",
    "    # Update pipeline status\n",
    "    pipeline_status['file_counts']['xmp_integrated'] = processed_files\n",
    "    pipeline_status['file_counts']['xmp_identifiers'] = added_identifiers\n",
    "    pipeline_status['file_counts']['derivations'] = added_derivations\n",
    "    pipeline_status['graph'] = g\n",
    "    \n",
    "    success = added_identifiers > 0 or processed_files > 0\n",
    "    log_step(\"XMP Integration\", success, f\"{processed_files:,} files, {added_identifiers:,} identifiers\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "def validate_xmp_integration():\n",
    "    \"\"\"Validate XMP integration with SPARQL queries\"\"\"\n",
    "    \n",
    "    if 'graph' not in pipeline_status:\n",
    "        return False\n",
    "    \n",
    "    g = pipeline_status['graph']\n",
    "    \n",
    "    print(\"üîç Validating XMP integration...\")\n",
    "    \n",
    "    # XMP-specific validation queries\n",
    "    xmp_queries = {\n",
    "        \"Files with XMP DocumentID\": \"\"\"\n",
    "            PREFIX premis: <http://www.loc.gov/premis/rdf/v3/>\n",
    "            SELECT (COUNT(?file) AS ?count) WHERE {\n",
    "                ?file premis:hasIdentifier ?id .\n",
    "                ?id premis:identifierType \"XMP DocumentID\" .\n",
    "            }\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Files with XMP InstanceID\": \"\"\"\n",
    "            PREFIX premis: <http://www.loc.gov/premis/rdf/v3/>\n",
    "            SELECT (COUNT(?file) AS ?count) WHERE {\n",
    "                ?file premis:hasIdentifier ?id .\n",
    "                ?id premis:identifierType \"XMP InstanceID\" .\n",
    "            }\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Derivation Relationships\": \"\"\"\n",
    "            PREFIX premis: <http://www.loc.gov/premis/rdf/v3/>\n",
    "            SELECT (COUNT(*) AS ?count) WHERE {\n",
    "                ?child premis:hasSource ?parent .\n",
    "            }\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    for query_name, query_text in xmp_queries.items():\n",
    "        try:\n",
    "            results = list(g.query(query_text))\n",
    "            if results:\n",
    "                count = int(results[0][0])\n",
    "                validation_results[query_name] = count\n",
    "                print(f\"   ‚úÖ {query_name}: {count:,}\")\n",
    "            else:\n",
    "                validation_results[query_name] = 0\n",
    "                print(f\"   ‚ùå {query_name}: 0\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {query_name}: Query failed - {e}\")\n",
    "    \n",
    "    # Store validation results\n",
    "    pipeline_status['xmp_validation'] = validation_results\n",
    "    \n",
    "    # Check if integration was successful\n",
    "    has_xmp_data = (validation_results.get(\"Files with XMP DocumentID\", 0) > 0 or\n",
    "                   validation_results.get(\"Files with XMP InstanceID\", 0) > 0)\n",
    "    \n",
    "    if has_xmp_data:\n",
    "        print(\"‚úÖ XMP integration validation PASSED\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  XMP integration validation: No XMP metadata found\")\n",
    "        return True  # Not an error if no XMP data exists\n",
    "\n",
    "# Execute XMP integration\n",
    "if pipeline_status.get('xmp_df') is not None:\n",
    "    integration_success = integrate_xmp_into_rdf()\n",
    "    \n",
    "    if integration_success:\n",
    "        validation_success = validate_xmp_integration()\n",
    "        pipeline_status['xmp_integration_success'] = integration_success\n",
    "        \n",
    "        print(f\"\\\\nüéØ XMP integration completed successfully!\")\n",
    "    else:\n",
    "        print(f\"\\\\n‚ùå XMP integration failed\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping XMP integration - no XMP data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729882b",
   "metadata": {},
   "source": [
    "## üíæ 10. Final RDF Export & Statistics\n",
    "\n",
    "Export der angereicherten RDF im Turtle-Format, Generierung umfassender Verarbeitungsstatistiken, Validierung der finalen Ausgabe und Erstellung von Backup-Dateien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04554700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# FINAL RDF EXPORT & COMPREHENSIVE STATISTICS\n",
    "# =====================================================\n",
    "\n",
    "def export_final_rdf():\n",
    "    \"\"\"Export final RDF with validation and backup\"\"\"\n",
    "    \n",
    "    if 'graph' not in pipeline_status:\n",
    "        print(\"‚ùå No graph available for export\")\n",
    "        return False\n",
    "    \n",
    "    g = pipeline_status['graph']\n",
    "    \n",
    "    print(\"üíæ Exporting final RDF to Turtle format...\")\n",
    "    \n",
    "    try:\n",
    "        # Serialize to Turtle with nice formatting\n",
    "        turtle_data = g.serialize(format='turtle')\n",
    "        \n",
    "        # Write to final output file\n",
    "        with open(rdf_output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(turtle_data)\n",
    "        \n",
    "        # Validate export\n",
    "        file_size_mb = rdf_output_path.stat().st_size / (1024 * 1024)\n",
    "        triple_count = len(g)\n",
    "        \n",
    "        print(f\"‚úÖ RDF exported successfully:\")\n",
    "        print(f\"   üìÅ File: {rdf_output_path}\")\n",
    "        print(f\"   üìä Size: {file_size_mb:.2f} MB\")\n",
    "        print(f\"   üìà Triples: {triple_count:,}\")\n",
    "        \n",
    "        # Test load validation\n",
    "        test_graph = Graph()\n",
    "        test_graph.parse(rdf_output_path, format='turtle')\n",
    "        \n",
    "        if len(test_graph) == triple_count:\n",
    "            print(f\"   ‚úÖ Export validation: Load test successful\")\n",
    "            log_step(\"RDF Export\", True, f\"{file_size_mb:.2f} MB, {triple_count:,} triples\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"   ‚ùå Export validation: Triple count mismatch\")\n",
    "            log_step(\"RDF Export\", False, \"Triple count mismatch in validation\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"RDF export failed: {e}\"\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        log_step(\"RDF Export\", False, error_msg)\n",
    "        return False\n",
    "\n",
    "def generate_pipeline_statistics():\n",
    "    \"\"\"Generate comprehensive pipeline statistics\"\"\"\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    execution_time = end_time - pipeline_status['start_time']\n",
    "    \n",
    "    stats = {\n",
    "        'execution_time': execution_time,\n",
    "        'start_time': pipeline_status['start_time'],\n",
    "        'end_time': end_time,\n",
    "        'steps_completed': pipeline_status['steps_completed'],\n",
    "        'steps_failed': pipeline_status['steps_failed'],\n",
    "        'file_counts': pipeline_status.get('file_counts', {}),\n",
    "        'errors': pipeline_status['errors']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\\\nüìä COMPREHENSIVE PIPELINE STATISTICS\")\n",
    "    print(f\"=\" * 60)\n",
    "    \n",
    "    # Execution Summary\n",
    "    print(f\"üïê Execution Time: {execution_time}\")\n",
    "    print(f\"üìÖ Started: {stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"üèÅ Completed: {stats['end_time'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print()\n",
    "    \n",
    "    # Step Summary\n",
    "    total_steps = len(stats['steps_completed']) + len(stats['steps_failed'])\n",
    "    success_rate = (len(stats['steps_completed']) / total_steps * 100) if total_steps > 0 else 0\n",
    "    \n",
    "    print(f\"üìã Pipeline Steps:\")\n",
    "    print(f\"   ‚úÖ Completed: {len(stats['steps_completed'])}\")\n",
    "    print(f\"   ‚ùå Failed: {len(stats['steps_failed'])}\")\n",
    "    print(f\"   üìà Success Rate: {success_rate:.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    # File Processing Summary\n",
    "    file_counts = stats['file_counts']\n",
    "    print(f\"üìÅ File Processing Summary:\")\n",
    "    \n",
    "    processing_stages = [\n",
    "        (\"Input Files Detected\", \"input_files\"),\n",
    "        (\"DROID Records Generated\", \"droid_records\"),\n",
    "        (\"Files Converted to RDF\", \"rdf_files\"),\n",
    "        (\"XMP Files Processed\", \"xmp_processed\"),\n",
    "        (\"XMP Files Integrated\", \"xmp_integrated\"),\n",
    "        (\"XMP Identifiers Added\", \"xmp_identifiers\"),\n",
    "        (\"Derivation Relations\", \"derivations\")\n",
    "    ]\n",
    "    \n",
    "    for label, key in processing_stages:\n",
    "        count = file_counts.get(key, 0)\n",
    "        if count > 0:\n",
    "            print(f\"   {label}: {count:,}\")\n",
    "    \n",
    "    # Error Processing\n",
    "    conversion_errors = file_counts.get('conversion_errors', 0)\n",
    "    total_processed = file_counts.get('rdf_files', 0) + conversion_errors\n",
    "    if total_processed > 0:\n",
    "        error_rate = (conversion_errors / total_processed) * 100\n",
    "        print(f\"   ‚ùå Conversion Errors: {conversion_errors:,} ({error_rate:.1f}%)\")\n",
    "    print()\n",
    "    \n",
    "    # RDF Statistics\n",
    "    if 'graph' in pipeline_status:\n",
    "        g = pipeline_status['graph']\n",
    "        print(f\"üìä RDF Graph Statistics:\")\n",
    "        print(f\"   üìà Total Triples: {len(g):,}\")\n",
    "        \n",
    "        # Calculate enrichment ratio\n",
    "        initial_triples = pipeline_status.get('initial_triples', 0)\n",
    "        if initial_triples > 0:\n",
    "            enrichment = ((len(g) - initial_triples) / initial_triples) * 100\n",
    "            print(f\"   üìà Enrichment: +{enrichment:.1f}% (+{len(g) - initial_triples:,} triples)\")\n",
    "        print()\n",
    "    \n",
    "    # Quality Metrics\n",
    "    print(f\"üéØ Quality Metrics:\")\n",
    "    if 'validation_results' in pipeline_status:\n",
    "        validation = pipeline_status['validation_results']\n",
    "        total_files = validation.get('Total Files', 0)\n",
    "        \n",
    "        if total_files > 0:\n",
    "            metrics = [\n",
    "                (\"Title Coverage\", \"Files with Titles\"),\n",
    "                (\"Identifier Coverage\", \"Files with Identifiers\"),  \n",
    "                (\"Format Coverage\", \"Files with Format Info\"),\n",
    "                (\"MD5 Hash Coverage\", \"Files with MD5 Hashes\")\n",
    "            ]\n",
    "            \n",
    "            for metric_name, key in metrics:\n",
    "                count = validation.get(key, 0)\n",
    "                if isinstance(count, int):\n",
    "                    coverage = (count / total_files) * 100\n",
    "                    print(f\"   {metric_name}: {coverage:.1f}% ({count:,}/{total_files:,})\")\n",
    "    \n",
    "    # XMP Integration Quality\n",
    "    if 'xmp_validation' in pipeline_status:\n",
    "        xmp_val = pipeline_status['xmp_validation']\n",
    "        xmp_doc_ids = xmp_val.get('Files with XMP DocumentID', 0)\n",
    "        xmp_inst_ids = xmp_val.get('Files with XMP InstanceID', 0)\n",
    "        derivations = xmp_val.get('Derivation Relationships', 0)\n",
    "        \n",
    "        if xmp_doc_ids > 0 or xmp_inst_ids > 0:\n",
    "            print(f\"   XMP DocumentIDs: {xmp_doc_ids:,} files\")\n",
    "            print(f\"   XMP InstanceIDs: {xmp_inst_ids:,} files\")\n",
    "            print(f\"   Derivation Links: {derivations:,} relationships\")\n",
    "    print()\n",
    "    \n",
    "    # Error Summary\n",
    "    if stats['errors']:\n",
    "        print(f\"‚ö†Ô∏è  Error Summary:\")\n",
    "        for error in stats['errors']:\n",
    "            print(f\"   ‚Ä¢ {error}\")\n",
    "        print()\n",
    "    \n",
    "    # Output Files\n",
    "    print(f\"üì§ Output Files:\")\n",
    "    print(f\"   üìÑ Final RDF: {rdf_output_path}\")\n",
    "    if rdf_output_path.exists():\n",
    "        size_mb = rdf_output_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"   üìä File Size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # Log file\n",
    "    log_files = list(Path('.').glob('dca_pipeline_*.log'))\n",
    "    if log_files:\n",
    "        latest_log = max(log_files, key=lambda p: p.stat().st_mtime)\n",
    "        print(f\"   üìã Log File: {latest_log}\")\n",
    "    print()\n",
    "    \n",
    "    # Success Assessment\n",
    "    critical_steps = ['Path Validation', 'DROID Analysis', 'RDF Graph Initialization', 'DROID to RDF Conversion']\n",
    "    critical_completed = [step for step in critical_steps if step in stats['steps_completed']]\n",
    "    \n",
    "    if len(critical_completed) == len(critical_steps):\n",
    "        print(f\"‚úÖ PIPELINE STATUS: SUCCESS\")\n",
    "        print(f\"   üéØ All critical steps completed successfully\")\n",
    "        print(f\"   üìà Ready for ETH DCA integration\")\n",
    "    else:\n",
    "        missing_steps = [step for step in critical_steps if step not in stats['steps_completed']]\n",
    "        print(f\"‚ö†Ô∏è  PIPELINE STATUS: PARTIAL SUCCESS\")\n",
    "        print(f\"   ‚ùå Missing critical steps: {', '.join(missing_steps)}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def create_pipeline_summary():\n",
    "    \"\"\"Create a summary report file\"\"\"\n",
    "    \n",
    "    stats = generate_pipeline_statistics()\n",
    "    \n",
    "    summary_path = rdf_output_path.parent / f\"pipeline_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    \n",
    "    try:\n",
    "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"DCA INTEGRATED PIPELINE - EXECUTION SUMMARY\\\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\\\n\\\\n\")\n",
    "            \n",
    "            f.write(f\"Project: {PROJECT_NAME}\\\\n\")\n",
    "            f.write(f\"Dataset: {dataset_to_analyze}\\\\n\")\n",
    "            f.write(f\"Execution Time: {stats['execution_time']}\\\\n\")\n",
    "            f.write(f\"Completed Steps: {len(stats['steps_completed'])}/{len(stats['steps_completed']) + len(stats['steps_failed'])}\\\\n\\\\n\")\n",
    "            \n",
    "            f.write(\"FILE PROCESSING:\\\\n\")\n",
    "            for key, value in stats['file_counts'].items():\n",
    "                f.write(f\"  {key}: {value:,}\\\\n\")\n",
    "            \n",
    "            if stats['errors']:\n",
    "                f.write(\"\\\\nERRORS:\\\\n\")\n",
    "                for error in stats['errors']:\n",
    "                    f.write(f\"  - {error}\\\\n\")\n",
    "        \n",
    "        print(f\"üìã Summary report saved: {summary_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Failed to create summary report: {e}\")\n",
    "\n",
    "# Execute final export and statistics\n",
    "print(\"üèÅ Finalizing integrated pipeline...\")\n",
    "\n",
    "export_success = export_final_rdf()\n",
    "pipeline_status['export_success'] = export_success\n",
    "\n",
    "if export_success:\n",
    "    print(\"\\\\nüéØ RDF Export completed successfully!\")\n",
    "\n",
    "# Generate comprehensive statistics\n",
    "stats = generate_pipeline_statistics()\n",
    "\n",
    "# Create summary report\n",
    "create_pipeline_summary()\n",
    "\n",
    "# Final status\n",
    "final_success = (\n",
    "    export_success and\n",
    "    len(pipeline_status['steps_failed']) == 0 and\n",
    "    'DROID to RDF Conversion' in pipeline_status['steps_completed']\n",
    ")\n",
    "\n",
    "if final_success:\n",
    "    print(f\"\\\\nüéä INTEGRATED PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"   üìÑ Final output ready: {rdf_output_path}\")\n",
    "    print(f\"   üîó Ready for ETH DCA integration and workflow visualization\")\n",
    "else:\n",
    "    print(f\"\\\\n‚ö†Ô∏è  PIPELINE COMPLETED WITH ISSUES\")\n",
    "    print(f\"   üìÑ Check logs and summary for details\")\n",
    "\n",
    "print(f\"\\\\nüìã Session log available in: dca_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
